{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### ***Deep Learning for Channel Coding via Neural Mutual Information Estimation***"
      ],
      "metadata": {
        "id": "v7j7gr_MwZAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code implements a **communication system using deep learning** techniques.\n",
        "\n",
        "- The main idea is to use an autoencoder to enhance the encoding and decoding of information. The autoencoder efficiently encodes data, which is then transmitted through a noisy channel. Finally, the received data is decoded by another deep learning model.\n",
        "\n",
        "- The process involves optimizing the models to minimize the bit error rate (BER) under varying signal-to-noise ratio (SNR) conditions. Additionally, neural networks are used to estimate mutual information to further improve the system's performance."
      ],
      "metadata": {
        "id": "DnQCqvaNwbd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import itertools\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib as mpl\n",
        "from scipy import special\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "assert sys.version_info >= (3, 5)\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# --------------------------------------------------------------------- Definition Of Constants ---------------------------------------------------------------------\n",
        "M = 16\n",
        "k = int(np.log2(M))\n",
        "n = 1\n",
        "TRAINING_SNR = 7\n",
        "BINARY_INP = True\n",
        "rayleigh = False"
      ],
      "metadata": {
        "id": "H31mj88kxCal"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The AutoEncoder class implements an end-to-end communication system using deep learning techniques.**\n",
        "\n",
        "It consists of an encoder, a channel model, and a decoder. The encoder maps input messages to codewords, the channel model simulates the transmission with noise, and the decoder attempts to reconstruct the original messages from the noisy codewords.\n",
        "\n",
        "This class supports both binary and non-binary inputs, and can operate with either an additive white `Gaussian noise (AWGN) channel` or a Rayleigh fading channel. Key functionalities include normalization of codewords, shaping the tensor to appropriate dimensions for transmission, and calculating the `bit error rate (BER)` for evaluating the performance.\n",
        "\n",
        "\n",
        "- Methods:\n",
        "\n",
        "   - __init__: Initializes the AutoEncoder with given parameters and constructs the encoder, channel, and decoder models.\n",
        "\n",
        "   - EbNo_to_noise: Converts the given Eb/No value in dB to the standard deviation of the noise.\n",
        "\n",
        "   - sample_Rayleigh_channel: Simulates the transmission over a Rayleigh fading channel with added noise.\n",
        "   \n",
        "   - random_sample: Generates random input messages for training or testing.\n",
        "\n",
        "   - B_Ber_m: Computes the Batch Bit Error Rate (BER) between the input and decoded messages.\n",
        "\n",
        "   - test_encoding: Tests the encoding process and visualizes the encoded messages in a 2D plot.\n"
      ],
      "metadata": {
        "id": "PumSl6bvy0Aj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder:\n",
        "    def __init__(self, M, n, training_snr, rayleigh=False, binary_input=True):\n",
        "        self.M = M\n",
        "        self.k = int(np.log2(M))\n",
        "        self.n = n\n",
        "        self.training_snr = training_snr\n",
        "        self.rayleigh = rayleigh\n",
        "        self.binary_input = binary_input\n",
        "\n",
        "        self.noise_std = self.EbNo_to_noise(training_snr)\n",
        "\n",
        "        # Define custom layers\n",
        "        self.norm_layer = layers.Lambda(lambda x: tf.divide(x, tf.sqrt(2 * tf.reduce_mean(tf.square(x)))))\n",
        "        self.shape_layer = layers.Lambda(lambda x: tf.reshape(x, shape=[-1, 2, n]))\n",
        "        self.shape_layer2 = layers.Lambda(lambda x: tf.reshape(x, shape=[-1, 2 * n]))\n",
        "        self.channel_layer = layers.Lambda(lambda x: x + tf.random.normal(tf.shape(x), mean=0.0, stddev=self.noise_std))\n",
        "\n",
        "        # Define the encoder model\n",
        "        if binary_input:\n",
        "            self.encoder = keras.models.Sequential([\n",
        "                keras.layers.InputLayer(input_shape=[k]),\n",
        "                keras.layers.Dense(2 * k, activation=\"elu\"),\n",
        "                keras.layers.Dense(2 * n, activation=None),\n",
        "                self.shape_layer,\n",
        "                self.norm_layer\n",
        "            ])\n",
        "        else:\n",
        "            self.encoder = keras.models.Sequential([\n",
        "                keras.layers.Embedding(M, M, embeddings_initializer='glorot_normal', input_length=1),\n",
        "                keras.layers.Dense(M, activation=\"elu\"),\n",
        "                keras.layers.Dense(2 * n, activation=None),\n",
        "                self.shape_layer,\n",
        "                self.norm_layer\n",
        "            ])\n",
        "\n",
        "        # Define the channel model\n",
        "        if rayleigh:\n",
        "            self.channel = keras.models.Sequential([\n",
        "                layers.Lambda(lambda x: self.sample_Rayleigh_channel(x, self.noise_std))\n",
        "            ])\n",
        "        else:\n",
        "            self.channel = keras.models.Sequential([\n",
        "                self.channel_layer\n",
        "            ])\n",
        "\n",
        "        self.decoder = keras.models.Sequential([\n",
        "            keras.layers.InputLayer(input_shape=[2, n]),\n",
        "            self.shape_layer2,\n",
        "            keras.layers.Dense(2 * k, activation=\"elu\"),\n",
        "            keras.layers.Dense(k, activation='sigmoid') if binary_input else keras.layers.Dense(M, activation=\"softmax\")\n",
        "        ])\n",
        "        self.autoencoder = keras.models.Sequential([self.encoder, self.channel, self.decoder])\n",
        "\n",
        "    def EbNo_to_noise(self, ebnodb):\n",
        "        '''Convert Eb/N0 (dB) to noise standard deviation.'''\n",
        "        ebno = 10**(ebnodb / 10)                                                                                        # Convert dB to linear scale\n",
        "        noise_std = 1 / np.sqrt(2 * (self.k / self.n) * ebno)                                                           # Compute noise std deviation\n",
        "        return noise_std\n",
        "\n",
        "    def sample_Rayleigh_channel(self, x, noise_std):\n",
        "        '''Simulate a Rayleigh channel with noise.'''\n",
        "        h_sample = (1 / np.sqrt(2)) * tf.sqrt(tf.random.normal(tf.shape(x))**2 + tf.random.normal(tf.shape(x))**2)      # Generate Rayleigh channel coefficients\n",
        "        z_sample = tf.random.normal(tf.shape(x), stddev=noise_std)                                                      # Generate noise\n",
        "        y_sample = x + tf.divide(z_sample, h_sample)                                                                    # Add noise to the signal\n",
        "        return tf.cast(y_sample, tf.float32)                                                                            # Return noisy signal\n",
        "\n",
        "    def random_sample(self, batch_size=32):\n",
        "        '''Generate random binary or integer samples.'''\n",
        "        if self.binary_input:\n",
        "            msg = np.random.randint(2, size=(batch_size, self.k))                                                       # Binary input\n",
        "        else:\n",
        "            msg = np.random.randint(self.M, size=(batch_size, 1))                                                       # Integer input\n",
        "        return msg\n",
        "\n",
        "    def B_Ber_m(self, input_msg, msg):\n",
        "        '''Compute the Batch Bit Error Rate (BBER).'''\n",
        "        batch_size = input_msg.shape[0]\n",
        "        if self.binary_input:\n",
        "            pred_error = tf.not_equal(input_msg, tf.round(msg))                                                         # Compare predictions with actual\n",
        "            pred_error_msg = tf.reduce_max(tf.cast(pred_error, tf.float32), axis=1)                                     # Max error per sample\n",
        "            bber = tf.reduce_mean(pred_error_msg)                                                                       # Mean error over batch\n",
        "        else:\n",
        "            pred_error = tf.not_equal(tf.reshape(input_msg, shape=(-1, batch_size)), tf.argmax(msg, 1))                 # Compare class predictions\n",
        "            bber = tf.reduce_mean(tf.cast(pred_error, tf.float32))                                                      # Mean error over batch\n",
        "        return bber\n",
        "\n",
        "    def test_encoding(self):\n",
        "        if self.binary_input:\n",
        "            inp = np.array([list(i) for i in itertools.product([0, 1], repeat=k)])\n",
        "        else:\n",
        "            inp = np.arange(0, self.M)\n",
        "        coding = self.encoder.predict(inp)\n",
        "        fig = plt.figure(figsize=(4, 4))\n",
        "        plt.plot(coding[:, 0], coding[:, 1], \"b.\")\n",
        "        plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "        plt.ylabel(\"$x_2$\", fontsize=18, rotation=0)\n",
        "        plt.grid(True)\n",
        "        plt.gca().set_ylim(-2, 2)\n",
        "        plt.gca().set_xlim(-2, 2)\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "MS1NG1pMxiAN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The NNFunction class defines a neural network-based function approximator using the TensorFlow Keras Model API.**\n",
        "\n",
        "It builds a fully connected **neural network** with a specified number of hidden layers, hidden dimension, and activation function. The primary purpose of this class is to compute a score for each pair of inputs from two batches. Key functionalities include tiling the input tensors, concatenating them, and passing through the neural network to obtain a score for each pair.\n",
        "\n",
        "\n",
        "- Methods:\n",
        "   - __init__: Initializes the `NNFunction` with the specified hidden dimension, number of layers, and activation function.\n",
        "\n",
        "   - call: Computes scores for each pair of inputs from two batches by tiling, concatenating, and passing through the neural network.\n"
      ],
      "metadata": {
        "id": "DoS0aS4wzYGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NNFunction(tf.keras.Model):\n",
        "    def __init__(self, hidden_dim, layers, activation, **extra_kwargs):\n",
        "        super(NNFunction, self).__init__()\n",
        "        self._f = tf.keras.Sequential(\n",
        "            [tf.keras.layers.Dense(hidden_dim, activation) for _ in range(layers)] +\n",
        "            [tf.keras.layers.Dense(1)]\n",
        "        )\n",
        "\n",
        "    def call(self, x, y):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        x_tiled = tf.tile(x[None, :], (batch_size, 1, 1))\n",
        "        y_tiled = tf.tile(y[:, None], (1, batch_size, 1))\n",
        "        xy_pairs = tf.reshape(tf.concat((x_tiled, y_tiled), axis=2), [batch_size * batch_size, -1])\n",
        "        scores = self._f(xy_pairs)\n",
        "        return tf.transpose(tf.reshape(scores, [batch_size, batch_size]))"
      ],
      "metadata": {
        "id": "be0Z4KphxkbJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The Trainer class is responsible for training and evaluating the AutoEncoder and NNFunction models.**\n",
        "\n",
        "It handles three types of training: `Mutual Information (MI) estimation`, decoder training, and encoder training. It also provides methods for plotting loss and testing the autoencoder's performance over a range of `signal-to-noise ratios (SNRs)`.\n",
        "\n",
        "\n",
        "- Methods:\n",
        "   - __init__: Initializes the Trainer with the AutoEncoder and NNFunction instances, setting up loss functions and metrics.\n",
        "\n",
        "   - MINE: Computes the Mutual Information Neural Estimator (MINE) score from the given pairwise scores.\n",
        "\n",
        "   - plot_loss: Prints loss and Batch Bit Error Rate (BBER) at regular intervals and optionally plots encoding results.\n",
        "\n",
        "   - plot_batch_loss: Prints interim results for the current epoch, including loss and BBER.\n",
        "\n",
        "   - train_mi: Trains the Mutual Information estimator using gradient descent, optimizing the MI score.\n",
        "\n",
        "   - train_decoder: Trains the autoencoder's decoder by minimizing the cross-entropy loss between input and predicted outputs.\n",
        "\n",
        "   - train_encoder: Trains the autoencoder's encoder by optimizing the MI score and includes an additional training step for the MI estimator.\n",
        "\n",
        "   - Test_AE: Evaluates the autoencoder's performance across a range of SNR values, computing the BBER for each SNR level.\n"
      ],
      "metadata": {
        "id": "CWjaNCGIzt8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, autoencoder, nn_function):\n",
        "        self.autoencoder = autoencoder\n",
        "        self.nn_function = nn_function\n",
        "        self.loss_fn = tf.keras.losses.BinaryCrossentropy() if autoencoder.binary_input else tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "        self.mean_loss = tf.keras.metrics.Mean()\n",
        "\n",
        "    def MINE(self, scores):\n",
        "        def marg(x):\n",
        "            batch_size = x.shape[0]\n",
        "            marg_ = tf.reduce_mean(tf.exp(x - tf.linalg.tensor_diag(np.inf * tf.ones(batch_size))))\n",
        "            return marg_ * ((batch_size * batch_size) / (batch_size * (batch_size - 1.)))\n",
        "\n",
        "        joint_term = tf.reduce_mean(tf.linalg.diag_part(scores))\n",
        "        marg_term = marg(scores)\n",
        "        return joint_term - tf.math.log(marg_term)\n",
        "\n",
        "    def plot_loss(self, step, epoch, mean_loss, X_batch, y_pred, plot_encoding):\n",
        "        template = 'Iteration: {}, Epoch: {}, Loss: {:.5f}, Batch_BER: {:.5f}'\n",
        "        if step % 10 == 0:\n",
        "            print(template.format(step, epoch, mean_loss.result(), self.autoencoder.B_Ber_m(X_batch, y_pred)))\n",
        "            if plot_encoding:\n",
        "                self.autoencoder.test_encoding()\n",
        "\n",
        "    def plot_batch_loss(self, epoch, mean_loss, X_batch, y_pred):\n",
        "        template_outer_loop = 'Interim result for Epoch: {}, Loss: {:.5f}, Batch_BER: {:.5f}'\n",
        "        print(template_outer_loop.format(epoch, mean_loss.result(), self.autoencoder.B_Ber_m(X_batch, y_pred)))\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------- TRAINING METHODS -------------------------------------------------------------------------------\n",
        "\n",
        "    def train_mi(self, n_epochs=5, n_steps=20, batch_size=200, learning_rate=0.005):\n",
        "        optimizer_mi = tf.keras.optimizers.Nadam(learning_rate=learning_rate)                                                                     # Initialize optimizer for mutual information estimation with specified learning rate\n",
        "        for epoch in range(1, n_epochs + 1):\n",
        "            print(\"Training in Epoch {}/{}\".format(epoch, n_epochs))\n",
        "            for step in range(1, n_steps + 1):\n",
        "                X_batch = self.autoencoder.random_sample(batch_size)                                                                              # Generate a batch of random samples\n",
        "                with tf.GradientTape() as tape:                                                                                                   # Compute gradients using a gradient tape\n",
        "                    x_enc = self.autoencoder.encoder(X_batch, training=True)                                                                      # Encode the batch of samples\n",
        "                    y_recv = self.autoencoder.channel(x_enc)                                                                                      # Pass the encoded samples through the channel\n",
        "                    x = tf.reshape(x_enc, shape=[batch_size, 2 * self.autoencoder.n])                                                             # Reshape tensors for mutual information estimation\n",
        "                    y = tf.reshape(y_recv, shape=[batch_size, 2 * self.autoencoder.n])\n",
        "                    score = self.nn_function(x, y)                                                                                                # Compute mutual information score\n",
        "                    loss = -self.MINE(score)                                                                                                      # Compute loss as negative MINE score\n",
        "                    gradients = tape.gradient(loss, self.nn_function.trainable_variables)                                                         # Compute gradients with respect to NNFunction variables\n",
        "                    optimizer_mi.apply_gradients(zip(gradients, self.nn_function.trainable_variables))                                            # Apply gradients to update NNFunction weights\n",
        "                mi_avg = -self.mean_loss(loss)                                                                                                    # Average mutual information loss over the steps\n",
        "            print('Epoch: {}, Mi is {}'.format(epoch, mi_avg))\n",
        "            self.mean_loss.reset_state()                                                                                                          # Reset the mean loss metric for the next epoch\n",
        "\n",
        "    def train_decoder(self, n_epochs=5, n_steps=20, batch_size=200, learning_rate=0.005, plot_encoding=True):\n",
        "        optimizer_ae = tf.keras.optimizers.Nadam(learning_rate=learning_rate)                                                                     # Initialize optimizer for the autoencoder decoder with specified learning rate\n",
        "        for epoch in range(1, n_epochs + 1):\n",
        "            print(\"Training Bob in Epoch {}/{}\".format(epoch, n_epochs))\n",
        "            for step in range(1, n_steps + 1):\n",
        "                X_batch = self.autoencoder.random_sample(batch_size)                                                                              # Generate a batch of random samples\n",
        "                with tf.GradientTape() as tape:                                                                                                   # Compute gradients using a gradient tape\n",
        "                    y_pred = self.autoencoder.autoencoder(X_batch, training=True)                                                                 # Pass the batch through the autoencoder\n",
        "                    loss = tf.reduce_mean(self.loss_fn(X_batch, y_pred))                                                                          # Compute loss using the loss function\n",
        "                    gradients = tape.gradient(loss, self.autoencoder.decoder.trainable_variables)                                                 # Compute gradients with respect to decoder variables\n",
        "                    optimizer_ae.apply_gradients(zip(gradients, self.autoencoder.decoder.trainable_variables))                                    # Apply gradients to update decoder weights\n",
        "                self.mean_loss(loss)                                                                                                              # Update the mean loss metric\n",
        "                self.plot_loss(step, epoch, self.mean_loss, X_batch, y_pred, plot_encoding)                                                       # Plot the current loss and encoding performance if required\n",
        "            self.plot_batch_loss(epoch, self.mean_loss, X_batch, y_pred)                                                                          # Plot the batch loss for the current epoch\n",
        "            self.mean_loss.reset_state()                                                                                                          # Reset the mean loss metric for the next epoch\n",
        "\n",
        "    def train_encoder(self, n_epochs=5, n_steps=20, batch_size=200, learning_rate=0.005):\n",
        "        optimizer_ae = tf.keras.optimizers.Nadam(learning_rate=learning_rate)                                                                     # Initialize optimizer for the autoencoder encoder with specified learning rate\n",
        "        optimizer_mi = tf.keras.optimizers.Nadam(learning_rate=0.005)                                                                             # Initialize optimizer for the mutual information estimator with a fixed learning rate\n",
        "        for epoch in range(1, n_epochs + 1):\n",
        "            print(\"Training Bob in Epoch {}/{}\".format(epoch, n_epochs))\n",
        "            for step in range(1, n_steps + 1):\n",
        "                X_batch = self.autoencoder.random_sample(batch_size)                                                                              # Generate a batch of random samples\n",
        "                with tf.GradientTape() as tape:                                                                                                   # Compute gradients using a gradient tape\n",
        "                    x_enc = self.autoencoder.encoder(X_batch, training=True)                                                                      # Encode the batch of samples\n",
        "                    y_recv = tf.identity(self.autoencoder.channel(x_enc))                                                                         # Pass the encoded samples through the channel\n",
        "                    x = tf.reshape(x_enc, shape=[batch_size, 2 * self.autoencoder.n])                                                             # Reshape the tensors for MI estimation\n",
        "                    y = tf.reshape(y_recv, shape=[batch_size, 2 * self.autoencoder.n])\n",
        "                    score = self.nn_function(x, y)                                                                                                # Compute mutual information score\n",
        "                    loss = -self.MINE(score)                                                                                                      # Compute loss as negative MINE score\n",
        "                    gradients = tape.gradient(loss, self.autoencoder.encoder.trainable_variables)                                                 # Compute gradients with respect to encoder variables\n",
        "                    optimizer_ae.apply_gradients(zip(gradients, self.autoencoder.encoder.trainable_variables))                                    # Apply gradients to update encoder weights\n",
        "                mi_avg = -self.mean_loss(loss)                                                                                                    # Average mutual information loss over the steps\n",
        "            with tf.GradientTape() as tape:                                                                                                       # Compute and update mutual information estimator\n",
        "                X_batch = self.autoencoder.random_sample(batch_size)                                                                              # Sample batch data\n",
        "                x_enc = self.autoencoder.encoder(X_batch, training=True)                                                                          # Encode batch data\n",
        "                y_recv = self.autoencoder.channel(x_enc)                                                                                          # Pass encoded data through channel\n",
        "                x = tf.reshape(x_enc, shape=[batch_size, 2 * self.autoencoder.n])                                                                 # Reshape encoded data\n",
        "                y = tf.reshape(y_recv, shape=[batch_size, 2 * self.autoencoder.n])                                                                # Reshape received data\n",
        "                score = self.nn_function(x, y)                                                                                                    # Compute mutual information score\n",
        "                loss = -self.MINE(score)                                                                                                          # Calculate loss as negative MINE score\n",
        "                gradients = tape.gradient(loss, self.nn_function.trainable_variables)                                                             # Compute gradients\n",
        "                optimizer_mi.apply_gradients(zip(gradients, self.nn_function.trainable_variables))                                                # Update weights\n",
        "\n",
        "    def Test_AE(self):\n",
        "        '''Calculate Bit Error for varying SNRs'''\n",
        "        snr_range = np.linspace(0, 15, 31)                                                                                                        # Define the range of SNR values to test\n",
        "        bber_vec = [None] * len(snr_range)                                                                                                        # Initialize a list to store Bit Error Rate (BBER) for each SNR\n",
        "        for db in range(len(snr_range)):\n",
        "            for it in range(1, 1000):\n",
        "                noise_std = self.autoencoder.EbNo_to_noise(snr_range[db])                                                                         # Convert SNR value to noise standard deviation\n",
        "                X_batch = self.autoencoder.random_sample(500)                                                                                     # Generate a batch of random samples\n",
        "                code_word = self.autoencoder.encoder(X_batch)                                                                                     # Encode the samples\n",
        "                if self.autoencoder.rayleigh:                                                                                                     # Simulate channel reception based on Rayleigh fading or AWGN\n",
        "                    rcvd_word = self.autoencoder.sample_Rayleigh_channel(code_word, noise_std)\n",
        "                else:\n",
        "                    rcvd_word = code_word + tf.random.normal(tf.shape(code_word), mean=0.0, stddev=noise_std)\n",
        "                dcoded_msg = self.autoencoder.decoder(rcvd_word)                                                                                  # Decode the received samples\n",
        "                bber = self.autoencoder.B_Ber_m(X_batch, dcoded_msg)                                                                              # Compute Bit Error Rate (BBER)\n",
        "                bber_avg = self.mean_loss(bber)                                                                                                   # Average BBER over trials\n",
        "            bber_vec[db] = bber_avg                                                                                                               # Store the average BBER for the current SNR\n",
        "            self.mean_loss.reset_state()                                                                                                          # Reset the mean loss metric\n",
        "            if (db % 6 == 0) & (db > 0):                                                                                                          # Print progress every 6 SNR steps\n",
        "                print(f'Progress: {db} of {30} parts')\n",
        "        return (snr_range, bber_vec)\n"
      ],
      "metadata": {
        "id": "7r-8DzujxnWJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This code demonstrates the training pipeline for an AutoEncoder model with `Mutual Information Neural Estimator (MINE)` and neural network-based components.\n",
        "\n",
        "- It sets up the NNFunction for MI estimation and the AutoEncoder for encoding and decoding. The training involves `optimizing MI`, followed by successive refinement of the encoder and decoder.\n",
        "\n",
        "- The process includes `training` epochs, testing encoding performance, and adjusting learning rates to improve the model's effectiveness and accuracy.\n"
      ],
      "metadata": {
        "id": "oVjYn_DW1E7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score_fn = NNFunction(hidden_dim=256, layers=2, activation='relu')                                                      # Initialize the NNFunction and AutoEncoder\n",
        "autoencoder = AutoEncoder(M=M, n=n, training_snr=TRAINING_SNR, rayleigh=rayleigh, binary_input=BINARY_INP)\n",
        "trainer = Trainer(autoencoder, score_fn)\n",
        "trainer.train_mi(n_epochs=1, n_steps=500, batch_size=64)                                                                # Train Mutual Information Estimator\n",
        "trainer.train_encoder(n_epochs=5, n_steps=400, batch_size=64, learning_rate=0.005)                                      # Train Encoder\n",
        "autoencoder.test_encoding()                                                                                             # Test Encoding\n",
        "trainer.train_encoder(n_epochs=5, n_steps=400, batch_size=64, learning_rate=0.001)                                      # Continue Training Encoder\n",
        "autoencoder.test_encoding()                                                                                             # Test Encoding\n",
        "trainer.train_decoder(n_epochs=5, n_steps=400, batch_size=500, learning_rate=0.015, plot_encoding=False)                # Train Decoder\n",
        "trainer.train_decoder(n_epochs=4, n_steps=400, batch_size=500, learning_rate=0.01, plot_encoding=False)                 # Continue Training Decoder\n",
        "trainer.train_decoder(n_epochs=1, n_steps=500, batch_size=500, learning_rate=0.005, plot_encoding=False)                # Final Training of Decoder"
      ],
      "metadata": {
        "id": "AekdY0ynw1L-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "de2e6e18-dede-4c5c-d0f6-09ecd7f26bf9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training in Epoch 1/1\n",
            "Epoch: 1, Mi is 1.4822927713394165\n",
            "Training Bob in Epoch 1/5\n",
            "Training Bob in Epoch 2/5\n",
            "Training Bob in Epoch 3/5\n",
            "Training Bob in Epoch 4/5\n",
            "Training Bob in Epoch 5/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAGCCAYAAADzM/Q6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAqmElEQVR4nO3deXDUdZ7/8Vcnhg4ZrkkISSAcwTBGRy5BMgEKwxkQD8TKqMwWR4VDB8bBUHK4s2BEJqWLQq2LHEskOMp6TMkhWkobA/wGOVaYlMdAhigSA0mUMCRLMnSa7v79kaU1koRvd47Ot/v5qEqF76c/n867P9XJi+/1aYvb7XYLAIAbCPF3AQAAcyAwAACGEBgAAEMIDACAIQQGAMAQAgMAYAiBAQAwhMAAABhCYAAADCEwAACGmCYwsrOzdeedd6pz587q0aOHpk2bpsLCwhuOe/vtt5WUlKTw8HANHDhQ77//fhtUCwCBxzSBceDAAS1cuFBHjhyRzWaTw+HQpEmTVF1d3eiYTz75RI888ogyMjL017/+VdOmTdO0adP0xRdftGHlABAYLGZdfPD7779Xjx49dODAAY0ZM6bBPg899JCqq6u1d+9eT9uvfvUrDRkyRJs2bWqrUgEgINzk7wJ8VVlZKUmKjIxstM/hw4eVmZlZry0tLU27du1qsL/dbpfdbvdsu1wuXbx4UVFRUbJYLM0vGgD8zO1263//93/Vs2dPhYR4d5DJlIHhcrm0ePFijRo1Srfffnuj/crKyhQTE1OvLSYmRmVlZQ32z87OVlZWVovWCgDt0bfffqv4+HivxpgyMBYuXKgvvvhCf/nLX1r0eVesWFFvj6SyslJ9+vTR3//+9yb3ZFCfw+FQfn6+xo4dq7CwMH+XYwrMmW+YN+9dvHhRv/jFL9S5c2evx5ouMBYtWqS9e/fq4MGDN0zH2NhYlZeX12srLy9XbGxsg/2tVqusVut17ZGRkYqKivK96CDjcDgUERGhqKgofokNYs58w7z5zpfD7Ka5SsrtdmvRokXauXOnPv74YyUkJNxwTEpKivLy8uq12Ww2paSktFaZABCwTLOHsXDhQu3YsUO7d+9W586dPechunbtqo4dO0qSZs6cqV69eik7O1uS9Pvf/1533XWXXnjhBU2dOlVvvPGGPv30U23ZssVvrwMAzMo0exgbN25UZWWlUlNTFRcX5/l68803PX2Ki4tVWlrq2R45cqR27NihLVu2aPDgwfrzn/+sXbt2NXmiHADQMNPsYRi5XWT//v3XtaWnpys9Pb0VKgKA4GKaPQwAgH8RGAAAQwgMAIAhBAYAwBACAwBgCIEBADCEwAAAGEJgAAAMITAAAIYQGAAAQwgMAIAhBAYAwBACAwBgCIEBADCEwAAAGEJgAAAMITAAAIYQGAAAQwgMAIAhBAYAwBACAwBgCIEBADCEwAAAGEJgAAAMITAAAIYQGAAAQwgMAIAhBAYAwBDTBMbBgwd17733qmfPnrJYLNq1a1eT/ffv3y+LxXLdV1lZWdsUDAABxjSBUV1drcGDB2vDhg1ejSssLFRpaannq0ePHq1UIQAEtpv8XYBRU6ZM0ZQpU7we16NHD3Xr1s1QX7vdLrvd7tmuqqqSJDkcDjkcDq9/drC6NlfMmXHMmW+YN+81Z65MExi+GjJkiOx2u26//XY9/fTTGjVqVKN9s7OzlZWVdV17fn6+IiIiWrPMgGSz2fxdgukwZ75h3oyrqanxeazF7Xa7W7CWNmGxWLRz505Nmzat0T6FhYXav3+/hg8fLrvdrq1bt+pPf/qTjh49qjvuuKPBMQ3tYfTu3VulpaWKiopq6ZcRsBwOh2w2myZOnKiwsDB/l2MKzJlvmDfvVVRUKC4uTpWVlerSpYtXYwN2D+OWW27RLbfc4tkeOXKkvvrqK61bt05/+tOfGhxjtVpltVqvaw8LC+PN6APmzXvMmW+YN+OaM0+mOendEkaMGKGioiJ/lwEAphRUgVFQUKC4uDh/lwEApmSaQ1KXL1+ut3dw5swZFRQUKDIyUn369NGKFSt07tw5vfrqq5Kk9evXKyEhQb/85S915coVbd26VR9//LH27dvnr5cAAKZmmsD49NNPNXbsWM92ZmamJGnWrFnKzc1VaWmpiouLPY/X1tZqyZIlOnfunCIiIjRo0CB99NFH9Z4DAGCcaQIjNTVVTV3QlZubW2976dKlWrp0aStXBQDBI6jOYQC4sZISKT+/7jvwYwQGAI+cHKlvX2ncuLrvOTn+rgjtCYEBQFLdHsX8+ZLLVbftckkLFrCngR8QGAAkSadP/xAW1zidErcu4RoCA4AkacAAKeQnfxFCQ6XERP/Ug/aHwAAgSYqPl7ZsqQsJqe775s117YBkostqAbS+jAwpLa3uMFRiImGB+ggMAPXExxMUaBiHpAAAhhAYAABDCAwAgCEEBgDAEAIDAGAIgQEAMITAAAAYQmAAAAwhMAAAhhAYAABDCAyYGp8OB7QdAgOmxafDAW2LwIAp8elwQNsjMGBKfDoc0PYIDJgSnw4HtD0CA6bEp8MBbY8PUIJp8elwQNsiMGBqfDoc0HY4JAUAMMQ0gXHw4EHde++96tmzpywWi3bt2nXDMfv379cdd9whq9WqxMRE5ebmtnqdABCoTBMY1dXVGjx4sDZs2GCo/5kzZzR16lSNHTtWBQUFWrx4sebOnasPP/ywlSsFgMBkmnMYU6ZM0ZQpUwz337RpkxISEvTCCy9Ikm699Vb95S9/0bp165SWltZaZQJAwDJNYHjr8OHDmjBhQr22tLQ0LV68uNExdrtddrvds11VVSVJcjgccjgcrVJnILo2V8yZccyZb5g37zVnrgI2MMrKyhQTE1OvLSYmRlVVVfrnP/+pjh07XjcmOztbWVlZ17Xn5+crIiKi1WoNVDabzd8lmA5z5hvmzbiamhqfxwZsYPhixYoVyszM9GxXVVWpd+/eGjt2rKKiovxYmbk4HA7ZbDZNnDhRYWFh/i7HFJgz3zBv3quoqPB5bMAGRmxsrMrLy+u1lZeXq0uXLg3uXUiS1WqV1Wq9rj0sLIw3ow+YN+8xZ75h3oxrzjyZ5iopb6WkpCgvL69em81mU0pKip8qAgBzM01gXL58WQUFBSooKJBUd9lsQUGBiouLJdUdTpo5c6an/6OPPqqvv/5aS5cu1alTp/Tyyy/rrbfe0hNPPOGP8gHA9EwTGJ9++qmGDh2qoUOHSpIyMzM1dOhQrVy5UpJUWlrqCQ9JSkhI0HvvvSebzabBgwfrhRde0NatW7mkFgB8ZJpzGKmpqXK73Y0+3tBd3KmpqfrrX//ailUBQPAwzR4GAMC/CAwAgCEEBgDAEAIDAGAIgQEAMITAAAAYQmAAAAwhMAAAhhAYAABDCAwAgCGmWRoEAH6spEQ6edKiCxfC/V1K0CAwAJhOTo40f77kct0ki2WSnE6n5s/3d1WBj0NSAEylpORaWNRtu90W/fa3oSop8W9dwYDAAGAqp0//EBbXOJ0WFRX5p55gQmAAMJUBA6SQn/zlCg11KzHRP/UEEwIDCCAlJVJ+vgL68Ex8vLRlixQaWrcdEuLSyy87FR/v37qCAYEBBIicHKlvX2ncuLrvOTn+rqj1ZGRI33wj2WxXtWWLTXPmNP7hamg5BAYQAH56ItjlkhYsCPw9jbvucqt79yv+LiVoEBhAAGj4RLA4EYwWRWAAAaDhE8HiRDBaFIEBBICfnggODZU2bxYngtGiuNMbCBAZGVJaWt1hqMREwgItj8AAAkh8PEGB1sMhKQCAIQQGAMAQAgMAYAiBAQAwxHSBsWHDBvXr10/h4eFKTk7WsWPHGu2bm5sri8VS7ys8nA9bAQBfmCow3nzzTWVmZmrVqlU6ceKEBg8erLS0NH333XeNjunSpYtKS0s9X2fPnm3DihFIgmFhP6AppgqMF198UfPmzdOcOXN02223adOmTYqIiNArr7zS6BiLxaLY2FjPV0xMTBtWjEARTAv7AY0xzX0YtbW1On78uFasWOFpCwkJ0YQJE3T48OFGx12+fFl9+/aVy+XSHXfcoT/+8Y/65S9/2WBfu90uu93u2a6qqpIkORwOORyOFnolge/aXDkcDpWUSEVFFiUmuk17f0Ddwn43yeWySLq2sJ9b48ZdbbHX9OM5g3HMm/eaM1emCYwLFy7I6XRet4cQExOjU6dONTjmlltu0SuvvKJBgwapsrJSa9eu1ciRI/Xll18qvoHf9OzsbGVlZV3Xnp+fr4iIiJZ5IUFk6dJCvfzyELndFlksbv32twWaOLHY32V57fPPu8vlGlWvzem06PXXj2rgwIoW/Vk2m61Fny9YMG/G1dTU+DzW4na7TbGQ/Pnz59WrVy998sknSklJ8bQvXbpUBw4c0NGjR2/4HA6HQ7feeqseeeQRrV69+rrHG9rD6N27t0pLSxUVFdUyLyQIOBwO/fd//z/Nnz/J879yqe5T0U6fbrn/lbeVkhIpMfGmVn0tDodDNptNEydOVFhYWMs8aRBg3rxXUVGhuLg4VVZWqkuXLl6NNc0eRvfu3RUaGqry8vJ67eXl5YqNjTX0HGFhYRo6dKiKGlnz2Wq1ymq1NjiON6N3Sks71fsDK9X9r/zs2TAlJPipKB8lJNQt7LdgQd2S4XUL+1mUkNDy7wnea75h3oxrzjyZ5qR3hw4dNGzYMOXl5XnaXC6X8vLy6u1xNMXpdOrzzz9XXFxca5WJ/xMXd1khIfV3Xs283Pa1T3jLz6/7npHh74qAtmeawJCkzMxM/dd//Ze2b9+ukydP6rHHHlN1dbXmzJkjSZo5c2a9k+LPPPOM9u3bp6+//lonTpzQv/zLv+js2bOaO3euv15C0Oje/Yo2bnQG1HLb8fFSaqq5XwPQHKY5JCVJDz30kL7//nutXLlSZWVlGjJkiD744APPifDi4mKF/OhTZP7xj39o3rx5Kisr089//nMNGzZMn3zyiW677TZ/vYSgMmeOW3ffzXLbQKAwVWBI0qJFi7Ro0aIGH9u/f3+97XXr1mndunVtUBUaw3LbQOAw1SEpAID/EBgAAEMIDACAIQQGAMAQAgMAYAiBAQAwhMAAABhCYAAADCEwAACGEBgAAEMIDACAIQQGAMAQnwLj0KFDslgsslgseuuttxrsc/ToUXXq1EkWi0VPPvlks4oEAPifT4ExatQo3XfffZKkVatWyel01nu8sLBQU6dOVXV1tWbNmqXnn3+++ZUCAPzK50NS2dnZCg0N1alTp/Taa6952s+fP6+0tDRVVFTonnvu0datW2WxWJp4JgCAGfgcGLfddptmz54tScrKypLD4dClS5c0efJknT17VqNHj9Zbb72lm24y3UduAAAa0KyT3llZWerYsaPOnDmjDRs26P7779fnn3+ugQMH6t1331XHjh1bqk4AgJ81KzB69eqlxx9/XJL0xBNP6ODBg+rXr58+/PBDdevW7br+J06cUGZmpgYPHqwuXbooOjpaY8aM0a5du5pTBgCgDTT7strHH3/c8znakZGR2rdvn+Li4hrs+/zzz2v79u0aMWKE/v3f/13/+q//KrvdrgceeEArV65sbikAgFbUrBMMV69e1YIFC+RyuSRJNTU1TR6G+t3vfqfc3FyFh4fXaxs9erSys7O1ePFiRUZGNqckAEAr8XkPw+12a+7cudq7d6+io6OVkJCgK1euaNWqVY2OGTVqVL2wkKTQ0FBNnz5dV69e1d///ndfywEAtDKfA2Pp0qXavn27OnXqpPfee09r1qyRJG3fvl1/+9vfvHqu8+fPS5Kio6N9LQcA0Mp8Coy1a9dq7dq1CgsL0zvvvKM777xTDz/8sAYNGiSn06kVK1YYfq5z585p27ZtSk5O1s033+xLOQCANuB1YLz66qtaunSpLBaLcnNzNXHiREmSxWLR6tWrJUl79uzRoUOHbvhcNTU1euCBB2S327VlyxZvSwEAtCGvAuP9999XRkaG3G63XnzxRc2YMaPe4/fdd5+Sk5MlScuWLWvyuWprazV9+nSdOHFCr7/+ugYNGuRl6QCAtmQ4MA4fPqz09HRdvXpVy5Yt0+LFixvsd+1cxqFDh7R79+4G+zgcDv3617/Wvn37lJOTo+nTp3tfOQCgTRm+rDYlJUXV1dU37Dd+/Hi53e5GH3c6nZoxY4Z2796tjRs3atasWUZLABDkSkqk06elAQOk+Hh/VxN82vTzMFwul2bNmqU///nPWrdunR599NG2/PEATCwnR+rbVxo3ru57To6/Kwo+bRoYTz75pF5//XWlpKSoe/fueu211+p9ff311zd8jg0bNqhfv34KDw9XcnKyjh071mT/t99+W0lJSQoPD9fAgQP1/vvvt9TLATxKSqT8/Lrv7Ul7rctbJSXS/PnS/90jLJdLWrDA/K/LbNp0Kdnjx49Lqjsfcvjw4ese37Ztm/r379/o+DfffFOZmZnatGmTkpOTtX79eqWlpamwsFA9evS4rv8nn3yiRx55RNnZ2brnnnu0Y8cOTZs2TSdOnNDtt9/eci8MQS0n54c/ZiEh0pYtUkaGv6tqv3X54vTpH8LiGqdT+uorPjqhTblNZMSIEe6FCxd6tp1Op7tnz57u7OzsBvv/+te/dk+dOrVeW3JysnvBggWGfl5lZaVbkvvChQu+Fx2Eamtr3bt27XLX1tb6u5RW9+23bndIiNst/fAVGlrX7o2WnrOWqqu9aOz1fP118LzXWsqFCxfcktyVlZVejzXNh1XU1tbq+PHj9W4KDAkJ0YQJExrcW5Hq9mQyMzPrtaWlpTW6Oq7dbpfdbvdsV1VVSaq7qsvhcDTzFQSPa3MVDHN28qRFLlf9XyOnUzp16qpiYhq/+OOnWnrOWqqu9iImRtq40aLf/jZUTqdFoaFuvfyyUzExwfNeaynNmSvTBMaFCxfkdDoVExNTrz0mJkanTp1qcExZWVmD/cvKyhrsn52draysrOva8/PzFRER4WPlwctms/m7hFZ34UK4LJZJcrt/ODQSEuLS2bN5ev/9K14/X0vNWUvX1R7ExEibN4ertPRniourVvfuV3RtuoLhvdZSampqfB5rmsBoCytWrKi3R1JVVaXevXtr7NixioqK8mNl5uJwOGSz2TRx4kSFhYX5u5xW53Q6f/I/X5dmzhzn1XO0xpy1RF3tXbC911pCRUWFz2NNExjdu3dXaGioysvL67WXl5crNja2wTGxsbFe9bdarbJarde1h4WF8Wb0QbDM2/z50t13S0VFUmKiRfHxvv9ateSctWRd7V2wvNdaQnPmqU0vq22ODh06aNiwYcrLy/O0uVwu5eXlKSUlpcExKSkp9fpLdbuujfUHfBUfL6Wmtr+bydprXTAnU/2XIzMzU7NmzdLw4cM1YsQIrV+/XtXV1ZozZ44kaebMmerVq5eys7MlSb///e9111136YUXXtDUqVP1xhtv6NNPP2WhQwDwgakC46GHHtL333+vlStXqqysTEOGDNEHH3zgObFdXFzs+bhYSRo5cqR27NihP/zhD3rqqac0YMAA7dq1i3swAMAHpgoMSVq0aJEWLVrU4GP79++/ri09PV3p6emtXBUABD7TnMMAAPgXgQHTC5T1koD2jsCAqbGCKdB2CAyYFiuYAm2LwIBpNbaCaVGRf+oBAh2BAdMaMKBu2e4fCw2VEhP9Uw8Q6AgMmFZ8fN1nPISG1m2HhkqbN3NXM9BaTHcfBvBjGRlSWtq19ZIIC6A1ERgwvfh4ggJoCxySAgAYQmAAAAwhMAAAhhAYAABDCAwAgCEEBgDAEAIDAGAIgQEAMITAAAAYQmAAAAwhMAAAhhAYAABDCAwAgCEEBgDAEAIDAGAIgQEAMITAAAAYQmAAAAwhMAAAhpgmMC5evKjf/OY36tKli7p166aMjAxdvny5yTGpqamyWCz1vh599NE2qhgAAstN/i7AqN/85jcqLS2VzWaTw+HQnDlzNH/+fO3YsaPJcfPmzdMzzzzj2Y6IiGjtUoFGlZRIp09LAwZI8fH+rgbwjikC4+TJk/rggw/0P//zPxo+fLgk6aWXXtLdd9+ttWvXqmfPno2OjYiIUGxsrKGfY7fbZbfbPdtVVVWSJIfDIYfD0YxXEFyuzRVzVt+2bRY99lioXC6LQkLc2rjRqTlz3JKYM18xb95rzlxZ3G63uwVraRWvvPKKlixZon/84x+etqtXryo8PFxvv/22HnjggQbHpaam6ssvv5Tb7VZsbKzuvfde/du//VujexlPP/20srKyrmvfsWMHeyZolgsXwjVv3iS53RZPW0iIS1u22NS9+xU/VoZgU1NToxkzZqiyslJdunTxaqwp9jDKysrUo0ePem033XSTIiMjVVZW1ui4GTNmqG/fvurZs6c+++wzLVu2TIWFhXrnnXca7L9ixQplZmZ6tquqqtS7d2+NHTtWUVFRLfNigoDD4ZDNZtPEiRMVFhbm73Lahf37LfXCQpJcrhD17Tted93lZs58xLx5r6Kiwuexfg2M5cuX67nnnmuyz8mTJ31+/vnz53v+PXDgQMXFxWn8+PH66quvdPPNN1/X32q1ymq1XtceFhbGm9EHzNsPbr1VCgmRXK4f2kJDpaSkm/TjKWLOfMO8GdecefJrYCxZskSzZ89usk///v0VGxur7777rl771atXdfHiRcPnJyQpOTlZklRUVNRgYACtJT5e2rJFWrBAcjrrwmLzZk58w1z8GhjR0dGKjo6+Yb+UlBRdunRJx48f17BhwyRJH3/8sVwulycEjCgoKJAkxcXF+VQv0BwZGVJamlRUJCUmEhYwH1Pch3Hrrbdq8uTJmjdvno4dO6ZDhw5p0aJFevjhhz1XSJ07d05JSUk6duyYJOmrr77S6tWrdfz4cX3zzTfas2ePZs6cqTFjxmjQoEH+fDkIYvHxUmoqYQFzMkVgSNLrr7+upKQkjR8/XnfffbdGjx6tLVu2eB53OBwqLCxUTU2NJKlDhw766KOPNGnSJCUlJWnJkiV68MEH9e677/rrJQCAqZniKilJioyMbPImvX79+unHVwj37t1bBw4caIvSgKDBjYfBzTR7GAD8KydH6ttXGjeu7ntOjr8rQlsjMADcUEmJNH/+D5cFu1x1V3yVlPi3LrQtAgM+KymR8vP5oxEMTp+ufw+JVHd5cFGRf+qBfxAY8AmHJ4LLgAF1Nx7+WGho3eXBCB4EBrzG4Yngc+3Gw9DQum1uPAxOprlKCu1HU4cn+AMSuLjxEAQGvHbt8MRP10Xi8ETgi48nKIIZh6TgNQ5PAMGJPQz4hMMTQPAhMOAzDk8AwYVDUgAAQwgMAIAhBAYAwBACA0GLpU0A7xAYCEosbQJ4j8BA0GFpE8A3BAaCDiuvAr4hMBB0WHkV8A2BgaDD0iaAb7jTG0GJpU0A7xEYCFosbQJ4h0NSAABDCAwAgCEEBgDAEAIDAGAIgQEAMITAAAAYYorAWLNmjUaOHKmIiAh169bN0Bi3262VK1cqLi5OHTt21IQJE3T69OnWLRQAApgpAqO2tlbp6el67LHHDI95/vnn9R//8R/atGmTjh49qp/97GdKS0vTlStXWrFSAAhcprhxLysrS5KUm5trqL/b7db69ev1hz/8Qffff78k6dVXX1VMTIx27dqlhx9+uLVKBYCAZYrA8NaZM2dUVlamCRMmeNq6du2q5ORkHT58uNHAsNvtstvtnu2qqipJksPhkMPhaN2iA8i1uWLOjGPOfMO8ea85cxWQgVFWViZJiomJqdceExPjeawh2dnZnr2ZH8vPz1dERETLFhkEbDabv0swHebMN8ybcTU1NT6P9VtgLF++XM8991yTfU6ePKmkpKQ2qkhasWKFMjMzPdtVVVXq3bu3xo4dq6ioqDarw+wcDodsNpsmTpyosLAwf5djCsyZb5g371VUVPg81m+BsWTJEs2ePbvJPv379/fpuWNjYyVJ5eXliouL87SXl5dryJAhjY6zWq2yWq3XtYeFhfFm9AHz5j3mzDfMm3HNmSe/BUZ0dLSio6Nb5bkTEhIUGxurvLw8T0BUVVXp6NGjXl1pBQD4gSkuqy0uLlZBQYGKi4vldDpVUFCggoICXb582dMnKSlJO3fulCRZLBYtXrxYzz77rPbs2aPPP/9cM2fOVM+ePTVt2jQ/vQoAMDdTnPReuXKltm/f7tkeOnSopLqT0ampqZKkwsJCVVZWevosXbpU1dXVmj9/vi5duqTRo0frgw8+UHh4eJvWDgCBwhSBkZube8N7MNxud71ti8WiZ555Rs8880wrVgYAwcMUh6QAAP5HYAAADCEwAACGEBgAAEMIDACAIQQGAMAQAgMAYAiBAQSIkhIpP7/uO9AaCAwgAOTkSH37SuPG1X3PyfF3RQhEBAZgciUl0vz5kstVt+1ySQsWsKeBlkdgACZ3+vQPYXGN0ykVFfmnHgQuAgMwuQEDpJCf/CaHhkqJif6pB4GLwABMLj5e2rKlLiSkuu+bN9e1Ay3JFKvVIjiVlNQdbhkwgD9+N5KRIaWl1R2GSkxkvtA62MNAu8RVP96Lj5dSUwkLtB4CA+0OV/0A7ROBgXaHq36A9onAQLvDVT9A+0RgoN3hqh+gfeIqKbRLXPUDtD8EBtqt+HiCAmhPOCQFADCEwAAAGEJgAAAMITAAAIYQGAAAQwgMAIAhpgiMNWvWaOTIkYqIiFC3bt0MjZk9e7YsFku9r8mTJ7duoQAQwExxH0Ztba3S09OVkpKiHC+WLZ08ebK2bdvm2bZara1RHgAEBVMERlZWliQpNzfXq3FWq1WxsbGtUBEABB9TBIav9u/frx49eujnP/+5xo0bp2effVZRUVGN9rfb7bLb7Z7tqqoqSZLD4ZDD4Wj1egPFtblizoxjznzDvHmvOXMVsIExefJkTZ8+XQkJCfrqq6/01FNPacqUKTp8+LBCr61q9xPZ2dmevZkfy8/PV0RERGuXHHBsNpu/SzAd5sw3zJtxNTU1Po+1uN1udwvWYtjy5cv13HPPNdnn5MmTSkpK8mzn5uZq8eLFunTpktc/7+uvv9bNN9+sjz76SOPHj2+wT0N7GL1791ZpaWmTeyaoz+FwyGazaeLEiQoLC/N3OabAnPmGefNeRUWF4uLiVFlZqS5dung11m97GEuWLNHs2bOb7NO/f/8W+3n9+/dX9+7dVVRU1GhgWK3WBk+Mh4WF8Wb0AfPmPebMN8ybcc2ZJ78FRnR0tKKjo9vs55WUlHiSFQDgPVPch1FcXKyCggIVFxfL6XSqoKBABQUFunz5sqdPUlKSdu7cKUm6fPmynnzySR05ckTffPON8vLydP/99ysxMVFpaWn+ehkAYGqmOOm9cuVKbd++3bM9dOhQSXUno1NTUyVJhYWFqqyslCSFhobqs88+0/bt23Xp0iX17NlTkyZN0urVq7kXAwB8ZIrAyM3NveE9GD8+d9+xY0d9+OGHrVwVAAQXUxySAgD4H4EBADCEwAAAGEJgAAAMITAAAIYQGAAAQwgMAIAhBAYAwBACAwBgCIEBADCEwAAAGEJgAAAMITAAAIYQGAAAQwgMAIAhBAYAwBACAwBgCIEBADCEwAAAGEJgAAAMITAAAIYQGAAAQwgMAIAhBAYAwBACAwBgCIEBADCEwAAAGEJgAAAMMUVgfPPNN8rIyFBCQoI6duyom2++WatWrVJtbW2T465cuaKFCxcqKipKnTp10oMPPqjy8vI2qhoAAospAuPUqVNyuVzavHmzvvzyS61bt06bNm3SU0891eS4J554Qu+++67efvttHThwQOfPn9f06dPbqGoACCw3+bsAIyZPnqzJkyd7tvv376/CwkJt3LhRa9eubXBMZWWlcnJytGPHDo0bN06StG3bNt166606cuSIfvWrX7VJ7QAQKEwRGA2prKxUZGRko48fP35cDodDEyZM8LQlJSWpT58+Onz4cIOBYbfbZbfb6/0MSbp48WILVh74HA6HampqVFFRobCwMH+XYwrMmW+YN+9d+3vmdru9HmvKwCgqKtJLL73U6N6FJJWVlalDhw7q1q1bvfaYmBiVlZU1OCY7O1tZWVnXtf/iF79oVr0A0N5UVFSoa9euXo3xa2AsX75czz33XJN9Tp48qaSkJM/2uXPnNHnyZKWnp2vevHktWs+KFSuUmZnp2b506ZL69u2r4uJiryc2mFVVVal379769ttv1aVLF3+XYwrMmW+YN+9VVlaqT58+TR6haYxfA2PJkiWaPXt2k3369+/v+ff58+c1duxYjRw5Ulu2bGlyXGxsrGpra3Xp0qV6exnl5eWKjY1tcIzVapXVar2uvWvXrrwZfdClSxfmzUvMmW+YN++FhHh/zZNfAyM6OlrR0dGG+p47d05jx47VsGHDtG3bthu+2GHDhiksLEx5eXl68MEHJUmFhYUqLi5WSkpKs2sHgGBjistqz507p9TUVPXp00dr167V999/r7KysnrnIs6dO6ekpCQdO3ZMUt1eQUZGhjIzM5Wfn6/jx49rzpw5SklJ4QopAPCBKU5622w2FRUVqaioSPHx8fUeu3am3+FwqLCwUDU1NZ7H1q1bp5CQED344IOy2+1KS0vTyy+/bPjnWq1WrVq1qsHDVGgc8+Y95sw3zJv3mjNnFrcv11YBAIKOKQ5JAQD8j8AAABhCYAAADCEwAACGEBgG+brEerBbs2aNRo4cqYiIiOuWacEPNmzYoH79+ik8PFzJycmey8PRsIMHD+ree+9Vz549ZbFYtGvXLn+X1O5lZ2frzjvvVOfOndWjRw9NmzZNhYWFXj0HgWGQr0usB7va2lqlp6frscce83cp7dabb76pzMxMrVq1SidOnNDgwYOVlpam7777zt+ltVvV1dUaPHiwNmzY4O9STOPAgQNauHChjhw5IpvNJofDoUmTJqm6utr4k7jhs+eff96dkJDg7zJMYdu2be6uXbv6u4x2acSIEe6FCxd6tp1Op7tnz57u7OxsP1ZlHpLcO3fu9HcZpvPdd9+5JbkPHDhgeAx7GM1woyXWgRupra3V8ePH6y3DHxISogkTJujw4cN+rAyB7trHN3jzN4zA8NG1JdYXLFjg71JgYhcuXJDT6VRMTEy99qaW4Qeay+VyafHixRo1apRuv/12w+OCPjCWL18ui8XS5NepU6fqjWnNJdbNwJc5A9B+LFy4UF988YXeeOMNr8aZYi2p1tSaS6wHKm/nDI3r3r27QkNDVV5eXq+9qWX4geZYtGiR9u7dq4MHD163Nt+NBH1gtOYS64HKmzlD0zp06KBhw4YpLy9P06ZNk1R3uCAvL0+LFi3yb3EIKG63W7/73e+0c+dO7d+/XwkJCV4/R9AHhlHXlljv27evZ4n1a/ifYOOKi4t18eJFFRcXy+l0qqCgQJKUmJioTp06+be4diIzM1OzZs3S8OHDNWLECK1fv17V1dWaM2eOv0trty5fvqyioiLP9pkzZ1RQUKDIyEj16dPHj5W1XwsXLtSOHTu0e/dude7c2XOOrGvXrurYsaOxJ2m9i7YCy7Zt29ySGvxC42bNmtXgnOXn5/u7tHblpZdecvfp08fdoUMH94gRI9xHjhzxd0ntWn5+foPvq1mzZvm7tHarsb9f27ZtM/wcLG8OADAkOA/CAwC8RmAAAAwhMAAAhhAYAABDCAwAgCEEBgDAEAIDAGAIgQEAMITAAAAYQmAAAAwhMAAAhhAYQCs7dOiQ54Ol3nrrrQb7HD16VJ06dZLFYtGTTz7ZxhUCxrD4INAG7r//fu3Zs0dJSUn64osvFBoa6nmssLBQo0aNUkVFhWbNmqVt27bJYrH4sVqgYexhAG0gOztboaGhOnXqlF577TVP+/nz55WWlqaKigrdc8892rp1K2GBdos9DKCNzJ07Vzk5OUpISFBhYaGqq6s1ZswYff755xo9erT27dtn/INsAD8gMIA2cu7cOQ0YMED//Oc/tW7dOu3cuVMHDx7UwIEDdfDgQXXr1s3fJQJN4pAU0EZ69eqlxx9/XJL0xBNP6ODBg+rXr58+/PDDBsPi8uXLevrpp3XPPfcoNjZWFotFs2fPbtuigR8hMIA29PjjjyskpO7XLjIyUvv27VNcXFyDfS9cuKCsrCydOHFCw4cPb8sygQbd5O8CgGBx9epVLViwQC6XS5JUU1PT5DmLuLg4lZSUqFevXrpy5QrnN+B37GEAbcDtdmvu3Lnau3evoqOjlZCQoCtXrmjVqlWNjrFarerVq1cbVgk0jcAA2sDSpUu1fft2derUSe+9957WrFkjSdq+fbv+9re/+bk6wBgCA2hla9eu1dq1axUWFqZ33nlHd955px5++GENGjRITqdTK1as8HeJgCEEBtCKXn31VS1dulQWi0W5ubmaOHGiJMlisWj16tWSpD179ujQoUP+LBMwhMAAWsn777+vjIwMud1uvfjii5oxY0a9x++77z4lJydLkpYtW+aPEgGvEBhAKzh8+LDS09N19epVLVu2TIsXL26w37VzGYcOHdLu3bvbsELAe1xWC7SClJQUVVdX37Df+PHjxWILMAv2MAAAhrCHAbRj//mf/6lLly7p6tWrkqTPPvtMzz77rCRpzJgxGjNmjD/LQ5Bh8UGgHevXr5/Onj3b4GOrVq3S008/3bYFIagRGAAAQziHAQAwhMAAABhCYAAADCEwAACGEBgAAEMIDACAIQQGAMAQAgMAYAiBAQAwhMAAABhCYAAADPn/kSt6FjbmNqQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Bob in Epoch 1/5\n",
            "Training Bob in Epoch 2/5\n",
            "Training Bob in Epoch 3/5\n",
            "Training Bob in Epoch 4/5\n",
            "Training Bob in Epoch 5/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Bob in Epoch 1/5\n",
            "Iteration: 10, Epoch: 1, Loss: -2.37077, Batch_BER: 0.76200\n",
            "Iteration: 20, Epoch: 1, Loss: -2.36353, Batch_BER: 0.68000\n",
            "Iteration: 30, Epoch: 1, Loss: -2.35650, Batch_BER: 0.61200\n",
            "Iteration: 40, Epoch: 1, Loss: -2.34960, Batch_BER: 0.66800\n",
            "Iteration: 50, Epoch: 1, Loss: -2.34278, Batch_BER: 0.64200\n",
            "Iteration: 60, Epoch: 1, Loss: -2.33605, Batch_BER: 0.62400\n",
            "Iteration: 70, Epoch: 1, Loss: -2.32937, Batch_BER: 0.61600\n",
            "Iteration: 80, Epoch: 1, Loss: -2.32272, Batch_BER: 0.69200\n",
            "Iteration: 90, Epoch: 1, Loss: -2.31614, Batch_BER: 0.64400\n",
            "Iteration: 100, Epoch: 1, Loss: -2.30960, Batch_BER: 0.57800\n",
            "Iteration: 110, Epoch: 1, Loss: -2.30310, Batch_BER: 0.58600\n",
            "Iteration: 120, Epoch: 1, Loss: -2.29661, Batch_BER: 0.63200\n",
            "Iteration: 130, Epoch: 1, Loss: -2.29018, Batch_BER: 0.58400\n",
            "Iteration: 140, Epoch: 1, Loss: -2.28378, Batch_BER: 0.59200\n",
            "Iteration: 150, Epoch: 1, Loss: -2.27742, Batch_BER: 0.61800\n",
            "Iteration: 160, Epoch: 1, Loss: -2.27108, Batch_BER: 0.60000\n",
            "Iteration: 170, Epoch: 1, Loss: -2.26480, Batch_BER: 0.60200\n",
            "Iteration: 180, Epoch: 1, Loss: -2.25853, Batch_BER: 0.61800\n",
            "Iteration: 190, Epoch: 1, Loss: -2.25229, Batch_BER: 0.61800\n",
            "Iteration: 200, Epoch: 1, Loss: -2.24609, Batch_BER: 0.61000\n",
            "Iteration: 210, Epoch: 1, Loss: -2.23991, Batch_BER: 0.62400\n",
            "Iteration: 220, Epoch: 1, Loss: -2.23378, Batch_BER: 0.58400\n",
            "Iteration: 230, Epoch: 1, Loss: -2.22768, Batch_BER: 0.62800\n",
            "Iteration: 240, Epoch: 1, Loss: -2.22160, Batch_BER: 0.56600\n",
            "Iteration: 250, Epoch: 1, Loss: -2.21557, Batch_BER: 0.59400\n",
            "Iteration: 260, Epoch: 1, Loss: -2.20956, Batch_BER: 0.61600\n",
            "Iteration: 270, Epoch: 1, Loss: -2.20358, Batch_BER: 0.59200\n",
            "Iteration: 280, Epoch: 1, Loss: -2.19762, Batch_BER: 0.60000\n",
            "Iteration: 290, Epoch: 1, Loss: -2.19170, Batch_BER: 0.51800\n",
            "Iteration: 300, Epoch: 1, Loss: -2.18581, Batch_BER: 0.58000\n",
            "Iteration: 310, Epoch: 1, Loss: -2.17995, Batch_BER: 0.55000\n",
            "Iteration: 320, Epoch: 1, Loss: -2.17411, Batch_BER: 0.56200\n",
            "Iteration: 330, Epoch: 1, Loss: -2.16830, Batch_BER: 0.54800\n",
            "Iteration: 340, Epoch: 1, Loss: -2.16252, Batch_BER: 0.53600\n",
            "Iteration: 350, Epoch: 1, Loss: -2.15678, Batch_BER: 0.54400\n",
            "Iteration: 360, Epoch: 1, Loss: -2.15106, Batch_BER: 0.49200\n",
            "Iteration: 370, Epoch: 1, Loss: -2.14538, Batch_BER: 0.53200\n",
            "Iteration: 380, Epoch: 1, Loss: -2.13971, Batch_BER: 0.50800\n",
            "Iteration: 390, Epoch: 1, Loss: -2.13407, Batch_BER: 0.52200\n",
            "Iteration: 400, Epoch: 1, Loss: -2.12847, Batch_BER: 0.45800\n",
            "Interim result for Epoch: 1, Loss: -2.12847, Batch_BER: 0.45800\n",
            "Training Bob in Epoch 2/5\n",
            "Iteration: 10, Epoch: 2, Loss: 0.33199, Batch_BER: 0.46400\n",
            "Iteration: 20, Epoch: 2, Loss: 0.33227, Batch_BER: 0.52000\n",
            "Iteration: 30, Epoch: 2, Loss: 0.33030, Batch_BER: 0.53400\n",
            "Iteration: 40, Epoch: 2, Loss: 0.32892, Batch_BER: 0.52600\n",
            "Iteration: 50, Epoch: 2, Loss: 0.32865, Batch_BER: 0.55400\n",
            "Iteration: 60, Epoch: 2, Loss: 0.32818, Batch_BER: 0.50400\n",
            "Iteration: 70, Epoch: 2, Loss: 0.32781, Batch_BER: 0.48800\n",
            "Iteration: 80, Epoch: 2, Loss: 0.32780, Batch_BER: 0.53200\n",
            "Iteration: 90, Epoch: 2, Loss: 0.32716, Batch_BER: 0.48400\n",
            "Iteration: 100, Epoch: 2, Loss: 0.32621, Batch_BER: 0.47400\n",
            "Iteration: 110, Epoch: 2, Loss: 0.32547, Batch_BER: 0.50800\n",
            "Iteration: 120, Epoch: 2, Loss: 0.32541, Batch_BER: 0.40800\n",
            "Iteration: 130, Epoch: 2, Loss: 0.32486, Batch_BER: 0.51800\n",
            "Iteration: 140, Epoch: 2, Loss: 0.32460, Batch_BER: 0.51400\n",
            "Iteration: 150, Epoch: 2, Loss: 0.32397, Batch_BER: 0.43200\n",
            "Iteration: 160, Epoch: 2, Loss: 0.32331, Batch_BER: 0.45600\n",
            "Iteration: 170, Epoch: 2, Loss: 0.32295, Batch_BER: 0.46400\n",
            "Iteration: 180, Epoch: 2, Loss: 0.32226, Batch_BER: 0.49000\n",
            "Iteration: 190, Epoch: 2, Loss: 0.32155, Batch_BER: 0.44800\n",
            "Iteration: 200, Epoch: 2, Loss: 0.32096, Batch_BER: 0.45800\n",
            "Iteration: 210, Epoch: 2, Loss: 0.32022, Batch_BER: 0.41800\n",
            "Iteration: 220, Epoch: 2, Loss: 0.31959, Batch_BER: 0.44000\n",
            "Iteration: 230, Epoch: 2, Loss: 0.31908, Batch_BER: 0.47800\n",
            "Iteration: 240, Epoch: 2, Loss: 0.31848, Batch_BER: 0.45200\n",
            "Iteration: 250, Epoch: 2, Loss: 0.31797, Batch_BER: 0.46200\n",
            "Iteration: 260, Epoch: 2, Loss: 0.31742, Batch_BER: 0.43200\n",
            "Iteration: 270, Epoch: 2, Loss: 0.31664, Batch_BER: 0.43000\n",
            "Iteration: 280, Epoch: 2, Loss: 0.31613, Batch_BER: 0.44200\n",
            "Iteration: 290, Epoch: 2, Loss: 0.31547, Batch_BER: 0.41000\n",
            "Iteration: 300, Epoch: 2, Loss: 0.31491, Batch_BER: 0.46200\n",
            "Iteration: 310, Epoch: 2, Loss: 0.31412, Batch_BER: 0.40600\n",
            "Iteration: 320, Epoch: 2, Loss: 0.31353, Batch_BER: 0.46800\n",
            "Iteration: 330, Epoch: 2, Loss: 0.31299, Batch_BER: 0.43000\n",
            "Iteration: 340, Epoch: 2, Loss: 0.31245, Batch_BER: 0.46000\n",
            "Iteration: 350, Epoch: 2, Loss: 0.31180, Batch_BER: 0.45000\n",
            "Iteration: 360, Epoch: 2, Loss: 0.31131, Batch_BER: 0.44000\n",
            "Iteration: 370, Epoch: 2, Loss: 0.31072, Batch_BER: 0.43200\n",
            "Iteration: 380, Epoch: 2, Loss: 0.31010, Batch_BER: 0.44200\n",
            "Iteration: 390, Epoch: 2, Loss: 0.30956, Batch_BER: 0.39600\n",
            "Iteration: 400, Epoch: 2, Loss: 0.30902, Batch_BER: 0.41000\n",
            "Interim result for Epoch: 2, Loss: 0.30902, Batch_BER: 0.41000\n",
            "Training Bob in Epoch 3/5\n",
            "Iteration: 10, Epoch: 3, Loss: 0.28297, Batch_BER: 0.41600\n",
            "Iteration: 20, Epoch: 3, Loss: 0.28404, Batch_BER: 0.41600\n",
            "Iteration: 30, Epoch: 3, Loss: 0.28619, Batch_BER: 0.37200\n",
            "Iteration: 40, Epoch: 3, Loss: 0.28529, Batch_BER: 0.40400\n",
            "Iteration: 50, Epoch: 3, Loss: 0.28368, Batch_BER: 0.38200\n",
            "Iteration: 60, Epoch: 3, Loss: 0.28335, Batch_BER: 0.42000\n",
            "Iteration: 70, Epoch: 3, Loss: 0.28405, Batch_BER: 0.40800\n",
            "Iteration: 80, Epoch: 3, Loss: 0.28319, Batch_BER: 0.38000\n",
            "Iteration: 90, Epoch: 3, Loss: 0.28257, Batch_BER: 0.41600\n",
            "Iteration: 100, Epoch: 3, Loss: 0.28219, Batch_BER: 0.34400\n",
            "Iteration: 110, Epoch: 3, Loss: 0.28229, Batch_BER: 0.41800\n",
            "Iteration: 120, Epoch: 3, Loss: 0.28173, Batch_BER: 0.41600\n",
            "Iteration: 130, Epoch: 3, Loss: 0.28163, Batch_BER: 0.39400\n",
            "Iteration: 140, Epoch: 3, Loss: 0.28097, Batch_BER: 0.37600\n",
            "Iteration: 150, Epoch: 3, Loss: 0.28030, Batch_BER: 0.39400\n",
            "Iteration: 160, Epoch: 3, Loss: 0.28009, Batch_BER: 0.41200\n",
            "Iteration: 170, Epoch: 3, Loss: 0.27950, Batch_BER: 0.38000\n",
            "Iteration: 180, Epoch: 3, Loss: 0.27946, Batch_BER: 0.39200\n",
            "Iteration: 190, Epoch: 3, Loss: 0.27898, Batch_BER: 0.38000\n",
            "Iteration: 200, Epoch: 3, Loss: 0.27885, Batch_BER: 0.39000\n",
            "Iteration: 210, Epoch: 3, Loss: 0.27845, Batch_BER: 0.37200\n",
            "Iteration: 220, Epoch: 3, Loss: 0.27781, Batch_BER: 0.36600\n",
            "Iteration: 230, Epoch: 3, Loss: 0.27721, Batch_BER: 0.37200\n",
            "Iteration: 240, Epoch: 3, Loss: 0.27707, Batch_BER: 0.40000\n",
            "Iteration: 250, Epoch: 3, Loss: 0.27691, Batch_BER: 0.39200\n",
            "Iteration: 260, Epoch: 3, Loss: 0.27656, Batch_BER: 0.39200\n",
            "Iteration: 270, Epoch: 3, Loss: 0.27624, Batch_BER: 0.34000\n",
            "Iteration: 280, Epoch: 3, Loss: 0.27583, Batch_BER: 0.38800\n",
            "Iteration: 290, Epoch: 3, Loss: 0.27564, Batch_BER: 0.38400\n",
            "Iteration: 300, Epoch: 3, Loss: 0.27544, Batch_BER: 0.36200\n",
            "Iteration: 310, Epoch: 3, Loss: 0.27513, Batch_BER: 0.37000\n",
            "Iteration: 320, Epoch: 3, Loss: 0.27481, Batch_BER: 0.36600\n",
            "Iteration: 330, Epoch: 3, Loss: 0.27451, Batch_BER: 0.35000\n",
            "Iteration: 340, Epoch: 3, Loss: 0.27407, Batch_BER: 0.33800\n",
            "Iteration: 350, Epoch: 3, Loss: 0.27378, Batch_BER: 0.39000\n",
            "Iteration: 360, Epoch: 3, Loss: 0.27354, Batch_BER: 0.36600\n",
            "Iteration: 370, Epoch: 3, Loss: 0.27322, Batch_BER: 0.34400\n",
            "Iteration: 380, Epoch: 3, Loss: 0.27296, Batch_BER: 0.36400\n",
            "Iteration: 390, Epoch: 3, Loss: 0.27282, Batch_BER: 0.35400\n",
            "Iteration: 400, Epoch: 3, Loss: 0.27243, Batch_BER: 0.34000\n",
            "Interim result for Epoch: 3, Loss: 0.27243, Batch_BER: 0.34000\n",
            "Training Bob in Epoch 4/5\n",
            "Iteration: 10, Epoch: 4, Loss: 0.25807, Batch_BER: 0.33600\n",
            "Iteration: 20, Epoch: 4, Loss: 0.26113, Batch_BER: 0.33200\n",
            "Iteration: 30, Epoch: 4, Loss: 0.26005, Batch_BER: 0.36000\n",
            "Iteration: 40, Epoch: 4, Loss: 0.26081, Batch_BER: 0.37400\n",
            "Iteration: 50, Epoch: 4, Loss: 0.26028, Batch_BER: 0.35400\n",
            "Iteration: 60, Epoch: 4, Loss: 0.26031, Batch_BER: 0.36600\n",
            "Iteration: 70, Epoch: 4, Loss: 0.25998, Batch_BER: 0.34000\n",
            "Iteration: 80, Epoch: 4, Loss: 0.25894, Batch_BER: 0.35200\n",
            "Iteration: 90, Epoch: 4, Loss: 0.25852, Batch_BER: 0.38200\n",
            "Iteration: 100, Epoch: 4, Loss: 0.25888, Batch_BER: 0.34800\n",
            "Iteration: 110, Epoch: 4, Loss: 0.25900, Batch_BER: 0.36000\n",
            "Iteration: 120, Epoch: 4, Loss: 0.25858, Batch_BER: 0.33400\n",
            "Iteration: 130, Epoch: 4, Loss: 0.25804, Batch_BER: 0.35400\n",
            "Iteration: 140, Epoch: 4, Loss: 0.25817, Batch_BER: 0.35200\n",
            "Iteration: 150, Epoch: 4, Loss: 0.25844, Batch_BER: 0.37000\n",
            "Iteration: 160, Epoch: 4, Loss: 0.25821, Batch_BER: 0.36800\n",
            "Iteration: 170, Epoch: 4, Loss: 0.25812, Batch_BER: 0.34200\n",
            "Iteration: 180, Epoch: 4, Loss: 0.25800, Batch_BER: 0.34400\n",
            "Iteration: 190, Epoch: 4, Loss: 0.25781, Batch_BER: 0.33200\n",
            "Iteration: 200, Epoch: 4, Loss: 0.25746, Batch_BER: 0.34600\n",
            "Iteration: 210, Epoch: 4, Loss: 0.25736, Batch_BER: 0.35000\n",
            "Iteration: 220, Epoch: 4, Loss: 0.25720, Batch_BER: 0.35800\n",
            "Iteration: 230, Epoch: 4, Loss: 0.25699, Batch_BER: 0.34200\n",
            "Iteration: 240, Epoch: 4, Loss: 0.25690, Batch_BER: 0.34800\n",
            "Iteration: 250, Epoch: 4, Loss: 0.25659, Batch_BER: 0.31600\n",
            "Iteration: 260, Epoch: 4, Loss: 0.25634, Batch_BER: 0.34000\n",
            "Iteration: 270, Epoch: 4, Loss: 0.25611, Batch_BER: 0.34800\n",
            "Iteration: 280, Epoch: 4, Loss: 0.25591, Batch_BER: 0.33600\n",
            "Iteration: 290, Epoch: 4, Loss: 0.25592, Batch_BER: 0.34000\n",
            "Iteration: 300, Epoch: 4, Loss: 0.25568, Batch_BER: 0.33200\n",
            "Iteration: 310, Epoch: 4, Loss: 0.25547, Batch_BER: 0.33000\n",
            "Iteration: 320, Epoch: 4, Loss: 0.25518, Batch_BER: 0.33200\n",
            "Iteration: 330, Epoch: 4, Loss: 0.25501, Batch_BER: 0.34600\n",
            "Iteration: 340, Epoch: 4, Loss: 0.25488, Batch_BER: 0.35800\n",
            "Iteration: 350, Epoch: 4, Loss: 0.25452, Batch_BER: 0.35200\n",
            "Iteration: 360, Epoch: 4, Loss: 0.25432, Batch_BER: 0.37800\n",
            "Iteration: 370, Epoch: 4, Loss: 0.25411, Batch_BER: 0.34600\n",
            "Iteration: 380, Epoch: 4, Loss: 0.25375, Batch_BER: 0.34000\n",
            "Iteration: 390, Epoch: 4, Loss: 0.25364, Batch_BER: 0.36200\n",
            "Iteration: 400, Epoch: 4, Loss: 0.25360, Batch_BER: 0.38600\n",
            "Interim result for Epoch: 4, Loss: 0.25360, Batch_BER: 0.38600\n",
            "Training Bob in Epoch 5/5\n",
            "Iteration: 10, Epoch: 5, Loss: 0.24668, Batch_BER: 0.34200\n",
            "Iteration: 20, Epoch: 5, Loss: 0.24848, Batch_BER: 0.33200\n",
            "Iteration: 30, Epoch: 5, Loss: 0.24685, Batch_BER: 0.31800\n",
            "Iteration: 40, Epoch: 5, Loss: 0.24582, Batch_BER: 0.32400\n",
            "Iteration: 50, Epoch: 5, Loss: 0.24497, Batch_BER: 0.36000\n",
            "Iteration: 60, Epoch: 5, Loss: 0.24467, Batch_BER: 0.33800\n",
            "Iteration: 70, Epoch: 5, Loss: 0.24506, Batch_BER: 0.31600\n",
            "Iteration: 80, Epoch: 5, Loss: 0.24477, Batch_BER: 0.35200\n",
            "Iteration: 90, Epoch: 5, Loss: 0.24490, Batch_BER: 0.35800\n",
            "Iteration: 100, Epoch: 5, Loss: 0.24478, Batch_BER: 0.29200\n",
            "Iteration: 110, Epoch: 5, Loss: 0.24393, Batch_BER: 0.32800\n",
            "Iteration: 120, Epoch: 5, Loss: 0.24358, Batch_BER: 0.32600\n",
            "Iteration: 130, Epoch: 5, Loss: 0.24328, Batch_BER: 0.36600\n",
            "Iteration: 140, Epoch: 5, Loss: 0.24297, Batch_BER: 0.34400\n",
            "Iteration: 150, Epoch: 5, Loss: 0.24280, Batch_BER: 0.34600\n",
            "Iteration: 160, Epoch: 5, Loss: 0.24258, Batch_BER: 0.31400\n",
            "Iteration: 170, Epoch: 5, Loss: 0.24219, Batch_BER: 0.33200\n",
            "Iteration: 180, Epoch: 5, Loss: 0.24223, Batch_BER: 0.33200\n",
            "Iteration: 190, Epoch: 5, Loss: 0.24222, Batch_BER: 0.35000\n",
            "Iteration: 200, Epoch: 5, Loss: 0.24175, Batch_BER: 0.35600\n",
            "Iteration: 210, Epoch: 5, Loss: 0.24154, Batch_BER: 0.34200\n",
            "Iteration: 220, Epoch: 5, Loss: 0.24157, Batch_BER: 0.32400\n",
            "Iteration: 230, Epoch: 5, Loss: 0.24145, Batch_BER: 0.32000\n",
            "Iteration: 240, Epoch: 5, Loss: 0.24117, Batch_BER: 0.32800\n",
            "Iteration: 250, Epoch: 5, Loss: 0.24128, Batch_BER: 0.30600\n",
            "Iteration: 260, Epoch: 5, Loss: 0.24120, Batch_BER: 0.32600\n",
            "Iteration: 270, Epoch: 5, Loss: 0.24106, Batch_BER: 0.34400\n",
            "Iteration: 280, Epoch: 5, Loss: 0.24079, Batch_BER: 0.33200\n",
            "Iteration: 290, Epoch: 5, Loss: 0.24065, Batch_BER: 0.33600\n",
            "Iteration: 300, Epoch: 5, Loss: 0.24045, Batch_BER: 0.31000\n",
            "Iteration: 310, Epoch: 5, Loss: 0.24008, Batch_BER: 0.31800\n",
            "Iteration: 320, Epoch: 5, Loss: 0.24006, Batch_BER: 0.30600\n",
            "Iteration: 330, Epoch: 5, Loss: 0.23979, Batch_BER: 0.31400\n",
            "Iteration: 340, Epoch: 5, Loss: 0.23957, Batch_BER: 0.30600\n",
            "Iteration: 350, Epoch: 5, Loss: 0.23953, Batch_BER: 0.30400\n",
            "Iteration: 360, Epoch: 5, Loss: 0.23958, Batch_BER: 0.34800\n",
            "Iteration: 370, Epoch: 5, Loss: 0.23930, Batch_BER: 0.32800\n",
            "Iteration: 380, Epoch: 5, Loss: 0.23900, Batch_BER: 0.31800\n",
            "Iteration: 390, Epoch: 5, Loss: 0.23878, Batch_BER: 0.34600\n",
            "Iteration: 400, Epoch: 5, Loss: 0.23857, Batch_BER: 0.34200\n",
            "Interim result for Epoch: 5, Loss: 0.23857, Batch_BER: 0.34200\n",
            "Training Bob in Epoch 1/4\n",
            "Iteration: 10, Epoch: 1, Loss: 0.23518, Batch_BER: 0.32000\n",
            "Iteration: 20, Epoch: 1, Loss: 0.23246, Batch_BER: 0.35200\n",
            "Iteration: 30, Epoch: 1, Loss: 0.23322, Batch_BER: 0.36000\n",
            "Iteration: 40, Epoch: 1, Loss: 0.23199, Batch_BER: 0.34000\n",
            "Iteration: 50, Epoch: 1, Loss: 0.23084, Batch_BER: 0.34000\n",
            "Iteration: 60, Epoch: 1, Loss: 0.23044, Batch_BER: 0.33600\n",
            "Iteration: 70, Epoch: 1, Loss: 0.23085, Batch_BER: 0.31800\n",
            "Iteration: 80, Epoch: 1, Loss: 0.23082, Batch_BER: 0.29600\n",
            "Iteration: 90, Epoch: 1, Loss: 0.23094, Batch_BER: 0.31200\n",
            "Iteration: 100, Epoch: 1, Loss: 0.23086, Batch_BER: 0.30400\n",
            "Iteration: 110, Epoch: 1, Loss: 0.23099, Batch_BER: 0.31000\n",
            "Iteration: 120, Epoch: 1, Loss: 0.23087, Batch_BER: 0.31800\n",
            "Iteration: 130, Epoch: 1, Loss: 0.23037, Batch_BER: 0.30400\n",
            "Iteration: 140, Epoch: 1, Loss: 0.23047, Batch_BER: 0.33600\n",
            "Iteration: 150, Epoch: 1, Loss: 0.23015, Batch_BER: 0.30400\n",
            "Iteration: 160, Epoch: 1, Loss: 0.23017, Batch_BER: 0.32800\n",
            "Iteration: 170, Epoch: 1, Loss: 0.22992, Batch_BER: 0.32800\n",
            "Iteration: 180, Epoch: 1, Loss: 0.22999, Batch_BER: 0.31200\n",
            "Iteration: 190, Epoch: 1, Loss: 0.22972, Batch_BER: 0.28200\n",
            "Iteration: 200, Epoch: 1, Loss: 0.22976, Batch_BER: 0.31200\n",
            "Iteration: 210, Epoch: 1, Loss: 0.22956, Batch_BER: 0.30600\n",
            "Iteration: 220, Epoch: 1, Loss: 0.22932, Batch_BER: 0.29600\n",
            "Iteration: 230, Epoch: 1, Loss: 0.22926, Batch_BER: 0.29400\n",
            "Iteration: 240, Epoch: 1, Loss: 0.22889, Batch_BER: 0.31200\n",
            "Iteration: 250, Epoch: 1, Loss: 0.22890, Batch_BER: 0.30400\n",
            "Iteration: 260, Epoch: 1, Loss: 0.22896, Batch_BER: 0.38400\n",
            "Iteration: 270, Epoch: 1, Loss: 0.22889, Batch_BER: 0.32800\n",
            "Iteration: 280, Epoch: 1, Loss: 0.22875, Batch_BER: 0.29600\n",
            "Iteration: 290, Epoch: 1, Loss: 0.22863, Batch_BER: 0.32800\n",
            "Iteration: 300, Epoch: 1, Loss: 0.22846, Batch_BER: 0.32000\n",
            "Iteration: 310, Epoch: 1, Loss: 0.22837, Batch_BER: 0.29000\n",
            "Iteration: 320, Epoch: 1, Loss: 0.22840, Batch_BER: 0.32800\n",
            "Iteration: 330, Epoch: 1, Loss: 0.22817, Batch_BER: 0.34200\n",
            "Iteration: 340, Epoch: 1, Loss: 0.22842, Batch_BER: 0.31800\n",
            "Iteration: 350, Epoch: 1, Loss: 0.22831, Batch_BER: 0.30400\n",
            "Iteration: 360, Epoch: 1, Loss: 0.22835, Batch_BER: 0.31200\n",
            "Iteration: 370, Epoch: 1, Loss: 0.22818, Batch_BER: 0.31000\n",
            "Iteration: 380, Epoch: 1, Loss: 0.22808, Batch_BER: 0.34200\n",
            "Iteration: 390, Epoch: 1, Loss: 0.22803, Batch_BER: 0.30200\n",
            "Iteration: 400, Epoch: 1, Loss: 0.22813, Batch_BER: 0.29200\n",
            "Interim result for Epoch: 1, Loss: 0.22813, Batch_BER: 0.29200\n",
            "Training Bob in Epoch 2/4\n",
            "Iteration: 10, Epoch: 2, Loss: 0.22085, Batch_BER: 0.27000\n",
            "Iteration: 20, Epoch: 2, Loss: 0.22091, Batch_BER: 0.27000\n",
            "Iteration: 30, Epoch: 2, Loss: 0.22385, Batch_BER: 0.30800\n",
            "Iteration: 40, Epoch: 2, Loss: 0.22468, Batch_BER: 0.30200\n",
            "Iteration: 50, Epoch: 2, Loss: 0.22448, Batch_BER: 0.31400\n",
            "Iteration: 60, Epoch: 2, Loss: 0.22492, Batch_BER: 0.28000\n",
            "Iteration: 70, Epoch: 2, Loss: 0.22493, Batch_BER: 0.29800\n",
            "Iteration: 80, Epoch: 2, Loss: 0.22542, Batch_BER: 0.30800\n",
            "Iteration: 90, Epoch: 2, Loss: 0.22571, Batch_BER: 0.34000\n",
            "Iteration: 100, Epoch: 2, Loss: 0.22514, Batch_BER: 0.30800\n",
            "Iteration: 110, Epoch: 2, Loss: 0.22499, Batch_BER: 0.32200\n",
            "Iteration: 120, Epoch: 2, Loss: 0.22491, Batch_BER: 0.28200\n",
            "Iteration: 130, Epoch: 2, Loss: 0.22431, Batch_BER: 0.28600\n",
            "Iteration: 140, Epoch: 2, Loss: 0.22438, Batch_BER: 0.31000\n",
            "Iteration: 150, Epoch: 2, Loss: 0.22430, Batch_BER: 0.31200\n",
            "Iteration: 160, Epoch: 2, Loss: 0.22414, Batch_BER: 0.31000\n",
            "Iteration: 170, Epoch: 2, Loss: 0.22425, Batch_BER: 0.31000\n",
            "Iteration: 180, Epoch: 2, Loss: 0.22410, Batch_BER: 0.28400\n",
            "Iteration: 190, Epoch: 2, Loss: 0.22394, Batch_BER: 0.28600\n",
            "Iteration: 200, Epoch: 2, Loss: 0.22358, Batch_BER: 0.26800\n",
            "Iteration: 210, Epoch: 2, Loss: 0.22367, Batch_BER: 0.31200\n",
            "Iteration: 220, Epoch: 2, Loss: 0.22375, Batch_BER: 0.30200\n",
            "Iteration: 230, Epoch: 2, Loss: 0.22382, Batch_BER: 0.29600\n",
            "Iteration: 240, Epoch: 2, Loss: 0.22387, Batch_BER: 0.33800\n",
            "Iteration: 250, Epoch: 2, Loss: 0.22385, Batch_BER: 0.31400\n",
            "Iteration: 260, Epoch: 2, Loss: 0.22380, Batch_BER: 0.27800\n",
            "Iteration: 270, Epoch: 2, Loss: 0.22364, Batch_BER: 0.28800\n",
            "Iteration: 280, Epoch: 2, Loss: 0.22368, Batch_BER: 0.31600\n",
            "Iteration: 290, Epoch: 2, Loss: 0.22362, Batch_BER: 0.30000\n",
            "Iteration: 300, Epoch: 2, Loss: 0.22372, Batch_BER: 0.28400\n",
            "Iteration: 310, Epoch: 2, Loss: 0.22365, Batch_BER: 0.29800\n",
            "Iteration: 320, Epoch: 2, Loss: 0.22349, Batch_BER: 0.32800\n",
            "Iteration: 330, Epoch: 2, Loss: 0.22335, Batch_BER: 0.27800\n",
            "Iteration: 340, Epoch: 2, Loss: 0.22313, Batch_BER: 0.26200\n",
            "Iteration: 350, Epoch: 2, Loss: 0.22292, Batch_BER: 0.29600\n",
            "Iteration: 360, Epoch: 2, Loss: 0.22293, Batch_BER: 0.27800\n",
            "Iteration: 370, Epoch: 2, Loss: 0.22296, Batch_BER: 0.30400\n",
            "Iteration: 380, Epoch: 2, Loss: 0.22287, Batch_BER: 0.29800\n",
            "Iteration: 390, Epoch: 2, Loss: 0.22268, Batch_BER: 0.28000\n",
            "Iteration: 400, Epoch: 2, Loss: 0.22256, Batch_BER: 0.30200\n",
            "Interim result for Epoch: 2, Loss: 0.22256, Batch_BER: 0.30200\n",
            "Training Bob in Epoch 3/4\n",
            "Iteration: 10, Epoch: 3, Loss: 0.21586, Batch_BER: 0.25600\n",
            "Iteration: 20, Epoch: 3, Loss: 0.21535, Batch_BER: 0.29000\n",
            "Iteration: 30, Epoch: 3, Loss: 0.21588, Batch_BER: 0.31800\n",
            "Iteration: 40, Epoch: 3, Loss: 0.21644, Batch_BER: 0.30200\n",
            "Iteration: 50, Epoch: 3, Loss: 0.21761, Batch_BER: 0.29200\n",
            "Iteration: 60, Epoch: 3, Loss: 0.21729, Batch_BER: 0.30800\n",
            "Iteration: 70, Epoch: 3, Loss: 0.21779, Batch_BER: 0.30800\n",
            "Iteration: 80, Epoch: 3, Loss: 0.21840, Batch_BER: 0.30600\n",
            "Iteration: 90, Epoch: 3, Loss: 0.21794, Batch_BER: 0.32800\n",
            "Iteration: 100, Epoch: 3, Loss: 0.21823, Batch_BER: 0.30400\n",
            "Iteration: 110, Epoch: 3, Loss: 0.21802, Batch_BER: 0.30600\n",
            "Iteration: 120, Epoch: 3, Loss: 0.21817, Batch_BER: 0.34800\n",
            "Iteration: 130, Epoch: 3, Loss: 0.21803, Batch_BER: 0.29400\n",
            "Iteration: 140, Epoch: 3, Loss: 0.21819, Batch_BER: 0.31200\n",
            "Iteration: 150, Epoch: 3, Loss: 0.21841, Batch_BER: 0.27600\n",
            "Iteration: 160, Epoch: 3, Loss: 0.21842, Batch_BER: 0.32000\n",
            "Iteration: 170, Epoch: 3, Loss: 0.21836, Batch_BER: 0.30200\n",
            "Iteration: 180, Epoch: 3, Loss: 0.21803, Batch_BER: 0.32000\n",
            "Iteration: 190, Epoch: 3, Loss: 0.21767, Batch_BER: 0.28000\n",
            "Iteration: 200, Epoch: 3, Loss: 0.21735, Batch_BER: 0.30000\n",
            "Iteration: 210, Epoch: 3, Loss: 0.21708, Batch_BER: 0.30000\n",
            "Iteration: 220, Epoch: 3, Loss: 0.21737, Batch_BER: 0.31800\n",
            "Iteration: 230, Epoch: 3, Loss: 0.21743, Batch_BER: 0.30800\n",
            "Iteration: 240, Epoch: 3, Loss: 0.21742, Batch_BER: 0.26200\n",
            "Iteration: 250, Epoch: 3, Loss: 0.21736, Batch_BER: 0.27000\n",
            "Iteration: 260, Epoch: 3, Loss: 0.21738, Batch_BER: 0.29400\n",
            "Iteration: 270, Epoch: 3, Loss: 0.21724, Batch_BER: 0.28800\n",
            "Iteration: 280, Epoch: 3, Loss: 0.21718, Batch_BER: 0.27400\n",
            "Iteration: 290, Epoch: 3, Loss: 0.21710, Batch_BER: 0.28200\n",
            "Iteration: 300, Epoch: 3, Loss: 0.21723, Batch_BER: 0.28800\n",
            "Iteration: 310, Epoch: 3, Loss: 0.21710, Batch_BER: 0.26000\n",
            "Iteration: 320, Epoch: 3, Loss: 0.21703, Batch_BER: 0.26600\n",
            "Iteration: 330, Epoch: 3, Loss: 0.21707, Batch_BER: 0.30600\n",
            "Iteration: 340, Epoch: 3, Loss: 0.21703, Batch_BER: 0.26200\n",
            "Iteration: 350, Epoch: 3, Loss: 0.21689, Batch_BER: 0.27200\n",
            "Iteration: 360, Epoch: 3, Loss: 0.21689, Batch_BER: 0.24800\n",
            "Iteration: 370, Epoch: 3, Loss: 0.21675, Batch_BER: 0.27800\n",
            "Iteration: 380, Epoch: 3, Loss: 0.21677, Batch_BER: 0.29000\n",
            "Iteration: 390, Epoch: 3, Loss: 0.21657, Batch_BER: 0.29200\n",
            "Iteration: 400, Epoch: 3, Loss: 0.21648, Batch_BER: 0.30800\n",
            "Interim result for Epoch: 3, Loss: 0.21648, Batch_BER: 0.30800\n",
            "Training Bob in Epoch 4/4\n",
            "Iteration: 10, Epoch: 4, Loss: 0.21602, Batch_BER: 0.27800\n",
            "Iteration: 20, Epoch: 4, Loss: 0.21167, Batch_BER: 0.26600\n",
            "Iteration: 30, Epoch: 4, Loss: 0.21344, Batch_BER: 0.28800\n",
            "Iteration: 40, Epoch: 4, Loss: 0.21324, Batch_BER: 0.34200\n",
            "Iteration: 50, Epoch: 4, Loss: 0.21277, Batch_BER: 0.29000\n",
            "Iteration: 60, Epoch: 4, Loss: 0.21325, Batch_BER: 0.29800\n",
            "Iteration: 70, Epoch: 4, Loss: 0.21291, Batch_BER: 0.27000\n",
            "Iteration: 80, Epoch: 4, Loss: 0.21286, Batch_BER: 0.28000\n",
            "Iteration: 90, Epoch: 4, Loss: 0.21299, Batch_BER: 0.27400\n",
            "Iteration: 100, Epoch: 4, Loss: 0.21343, Batch_BER: 0.28200\n",
            "Iteration: 110, Epoch: 4, Loss: 0.21312, Batch_BER: 0.28200\n",
            "Iteration: 120, Epoch: 4, Loss: 0.21273, Batch_BER: 0.32800\n",
            "Iteration: 130, Epoch: 4, Loss: 0.21329, Batch_BER: 0.28000\n",
            "Iteration: 140, Epoch: 4, Loss: 0.21363, Batch_BER: 0.31600\n",
            "Iteration: 150, Epoch: 4, Loss: 0.21339, Batch_BER: 0.32600\n",
            "Iteration: 160, Epoch: 4, Loss: 0.21347, Batch_BER: 0.30200\n",
            "Iteration: 170, Epoch: 4, Loss: 0.21310, Batch_BER: 0.29400\n",
            "Iteration: 180, Epoch: 4, Loss: 0.21323, Batch_BER: 0.28200\n",
            "Iteration: 190, Epoch: 4, Loss: 0.21293, Batch_BER: 0.25400\n",
            "Iteration: 200, Epoch: 4, Loss: 0.21315, Batch_BER: 0.28000\n",
            "Iteration: 210, Epoch: 4, Loss: 0.21311, Batch_BER: 0.30000\n",
            "Iteration: 220, Epoch: 4, Loss: 0.21283, Batch_BER: 0.28000\n",
            "Iteration: 230, Epoch: 4, Loss: 0.21300, Batch_BER: 0.29400\n",
            "Iteration: 240, Epoch: 4, Loss: 0.21324, Batch_BER: 0.25600\n",
            "Iteration: 250, Epoch: 4, Loss: 0.21312, Batch_BER: 0.30400\n",
            "Iteration: 260, Epoch: 4, Loss: 0.21327, Batch_BER: 0.31200\n",
            "Iteration: 270, Epoch: 4, Loss: 0.21331, Batch_BER: 0.32000\n",
            "Iteration: 280, Epoch: 4, Loss: 0.21329, Batch_BER: 0.27800\n",
            "Iteration: 290, Epoch: 4, Loss: 0.21310, Batch_BER: 0.29200\n",
            "Iteration: 300, Epoch: 4, Loss: 0.21310, Batch_BER: 0.27200\n",
            "Iteration: 310, Epoch: 4, Loss: 0.21320, Batch_BER: 0.29800\n",
            "Iteration: 320, Epoch: 4, Loss: 0.21332, Batch_BER: 0.28000\n",
            "Iteration: 330, Epoch: 4, Loss: 0.21332, Batch_BER: 0.27200\n",
            "Iteration: 340, Epoch: 4, Loss: 0.21334, Batch_BER: 0.27400\n",
            "Iteration: 350, Epoch: 4, Loss: 0.21323, Batch_BER: 0.26600\n",
            "Iteration: 360, Epoch: 4, Loss: 0.21306, Batch_BER: 0.27200\n",
            "Iteration: 370, Epoch: 4, Loss: 0.21298, Batch_BER: 0.26600\n",
            "Iteration: 380, Epoch: 4, Loss: 0.21298, Batch_BER: 0.28400\n",
            "Iteration: 390, Epoch: 4, Loss: 0.21297, Batch_BER: 0.29400\n",
            "Iteration: 400, Epoch: 4, Loss: 0.21303, Batch_BER: 0.26800\n",
            "Interim result for Epoch: 4, Loss: 0.21303, Batch_BER: 0.26800\n",
            "Training Bob in Epoch 1/1\n",
            "Iteration: 10, Epoch: 1, Loss: 0.20840, Batch_BER: 0.25000\n",
            "Iteration: 20, Epoch: 1, Loss: 0.21065, Batch_BER: 0.25600\n",
            "Iteration: 30, Epoch: 1, Loss: 0.21060, Batch_BER: 0.30000\n",
            "Iteration: 40, Epoch: 1, Loss: 0.20956, Batch_BER: 0.28400\n",
            "Iteration: 50, Epoch: 1, Loss: 0.21006, Batch_BER: 0.30400\n",
            "Iteration: 60, Epoch: 1, Loss: 0.21044, Batch_BER: 0.29800\n",
            "Iteration: 70, Epoch: 1, Loss: 0.21073, Batch_BER: 0.32200\n",
            "Iteration: 80, Epoch: 1, Loss: 0.21015, Batch_BER: 0.28000\n",
            "Iteration: 90, Epoch: 1, Loss: 0.21103, Batch_BER: 0.29200\n",
            "Iteration: 100, Epoch: 1, Loss: 0.21109, Batch_BER: 0.30800\n",
            "Iteration: 110, Epoch: 1, Loss: 0.21135, Batch_BER: 0.26800\n",
            "Iteration: 120, Epoch: 1, Loss: 0.21160, Batch_BER: 0.31800\n",
            "Iteration: 130, Epoch: 1, Loss: 0.21132, Batch_BER: 0.27600\n",
            "Iteration: 140, Epoch: 1, Loss: 0.21145, Batch_BER: 0.28200\n",
            "Iteration: 150, Epoch: 1, Loss: 0.21130, Batch_BER: 0.23000\n",
            "Iteration: 160, Epoch: 1, Loss: 0.21088, Batch_BER: 0.25000\n",
            "Iteration: 170, Epoch: 1, Loss: 0.21094, Batch_BER: 0.30800\n",
            "Iteration: 180, Epoch: 1, Loss: 0.21077, Batch_BER: 0.26400\n",
            "Iteration: 190, Epoch: 1, Loss: 0.21042, Batch_BER: 0.26800\n",
            "Iteration: 200, Epoch: 1, Loss: 0.21054, Batch_BER: 0.28800\n",
            "Iteration: 210, Epoch: 1, Loss: 0.21043, Batch_BER: 0.27600\n",
            "Iteration: 220, Epoch: 1, Loss: 0.21045, Batch_BER: 0.27800\n",
            "Iteration: 230, Epoch: 1, Loss: 0.21057, Batch_BER: 0.29200\n",
            "Iteration: 240, Epoch: 1, Loss: 0.21038, Batch_BER: 0.27000\n",
            "Iteration: 250, Epoch: 1, Loss: 0.21017, Batch_BER: 0.25800\n",
            "Iteration: 260, Epoch: 1, Loss: 0.21003, Batch_BER: 0.29600\n",
            "Iteration: 270, Epoch: 1, Loss: 0.21025, Batch_BER: 0.29400\n",
            "Iteration: 280, Epoch: 1, Loss: 0.21030, Batch_BER: 0.28400\n",
            "Iteration: 290, Epoch: 1, Loss: 0.21030, Batch_BER: 0.26600\n",
            "Iteration: 300, Epoch: 1, Loss: 0.21027, Batch_BER: 0.24800\n",
            "Iteration: 310, Epoch: 1, Loss: 0.21023, Batch_BER: 0.27400\n",
            "Iteration: 320, Epoch: 1, Loss: 0.21016, Batch_BER: 0.29600\n",
            "Iteration: 330, Epoch: 1, Loss: 0.21013, Batch_BER: 0.27400\n",
            "Iteration: 340, Epoch: 1, Loss: 0.21017, Batch_BER: 0.24600\n",
            "Iteration: 350, Epoch: 1, Loss: 0.21030, Batch_BER: 0.29600\n",
            "Iteration: 360, Epoch: 1, Loss: 0.21035, Batch_BER: 0.31200\n",
            "Iteration: 370, Epoch: 1, Loss: 0.21033, Batch_BER: 0.25800\n",
            "Iteration: 380, Epoch: 1, Loss: 0.21032, Batch_BER: 0.25800\n",
            "Iteration: 390, Epoch: 1, Loss: 0.21026, Batch_BER: 0.30800\n",
            "Iteration: 400, Epoch: 1, Loss: 0.21022, Batch_BER: 0.26200\n",
            "Iteration: 410, Epoch: 1, Loss: 0.21027, Batch_BER: 0.28400\n",
            "Iteration: 420, Epoch: 1, Loss: 0.21018, Batch_BER: 0.25600\n",
            "Iteration: 430, Epoch: 1, Loss: 0.21020, Batch_BER: 0.30600\n",
            "Iteration: 440, Epoch: 1, Loss: 0.21025, Batch_BER: 0.23600\n",
            "Iteration: 450, Epoch: 1, Loss: 0.21016, Batch_BER: 0.22800\n",
            "Iteration: 460, Epoch: 1, Loss: 0.21014, Batch_BER: 0.28400\n",
            "Iteration: 470, Epoch: 1, Loss: 0.20999, Batch_BER: 0.25800\n",
            "Iteration: 480, Epoch: 1, Loss: 0.21011, Batch_BER: 0.31800\n",
            "Iteration: 490, Epoch: 1, Loss: 0.21008, Batch_BER: 0.26400\n",
            "Iteration: 500, Epoch: 1, Loss: 0.21003, Batch_BER: 0.27400\n",
            "Interim result for Epoch: 1, Loss: 0.21003, Batch_BER: 0.27400\n"
          ]
        }
      ]
    }
  ]
}